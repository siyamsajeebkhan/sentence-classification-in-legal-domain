{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2388e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\siyam\\Desktop\\Legal DS\\project\\venv\\Lib\\site-packages\")\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.language import Language\n",
    "from luima_sbd import sbd_utils as luima\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d6b82",
   "metadata": {},
   "source": [
    "# Phase 3.1: Splitting the unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a245f496",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNLABELED_DATA_PATH = Path(\"../Data/ldsi_w2021-20220221T223611Z-001/ldsi_w2021/unlabeled/unlabeled\")\n",
    "unlabeled_corpus = sorted(os.listdir(UNLABELED_DATA_PATH))\n",
    "len(unlabeled_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0158e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = open(UNLABELED_DATA_PATH/ Path('0603946.txt'), 'r').read().encode('latin-1', 'ignore')\n",
    "# test_file = test_file.decode('latin-1').strip()\n",
    "# len(luima.text2sentences(test_file, offsets=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe89091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = open(UNLABELED_DATA_PATH/ Path('0603946.txt'), 'r').read()\n",
    "# arr = bytes(test_file, 'utf-8')\n",
    "# arr = arr.decode('latin-1').strip()\n",
    "# len(luima.text2sentences(arr, offsets=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ac515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unlab_segments_by_doc = []\n",
    "# for file in tqdm(unlabeled_corpus):\n",
    "#     text = open(UNLABELED_DATA_PATH/file).read()\n",
    "#     byte_text = bytes(text, 'utf-8')\n",
    "#     text = byte_text.decode('latin-1').strip()\n",
    "    \n",
    "#     doc = luima.text2sentences(text, offsets=False)\n",
    "    \n",
    "#     seg_data = {\n",
    "#         'doc_name': file,\n",
    "#         'no_of_sentences': len(doc),\n",
    "#         'sentences': doc\n",
    "#     }\n",
    "#     unlab_segments_by_doc.append(seg_data)\n",
    "\n",
    "\n",
    "# with open(Path('../Data/')/'unlab_segments_by_doc', 'wb') as handle:\n",
    "#     pickle.dump(unlab_segments_by_doc, handle)\n",
    "\n",
    "# total_sentences = 0\n",
    "# for seg in unlab_segments_by_doc:\n",
    "#     total_sentences += seg['no_of_sentences']\n",
    "    \n",
    "# print(f\"Total sentences in the unlabeled corpus: {total_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79817f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Process\n",
    "# from multiprocessing import Pool\n",
    "# import os\n",
    "# import utils\n",
    "# from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5ab53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     start = perf_counter()\n",
    "#     with Pool(5) as p:\n",
    "# #         print(p.map(f, [1, 2, 3]))\n",
    "#         p.map(utils.segment_unlabeled_corpus, unlabeled_corpus[0:5])\n",
    "#     end = perf_counter()\n",
    "#     print(f\"time needed for 5 files: {end-start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4b401",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(Path('../Data/')/'unlab_segments_by_doc', 'rb') as handle:\n",
    "    unlab_segments_by_doc = pickle.load(handle)\n",
    "print(len(unlab_segments_by_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c337fe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlab_segments_by_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c30ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([unlab_seg['no_of_sentences'] for unlab_seg in unlab_segments_by_doc])\n",
    "df.columns = ['no_of_sentences']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885a521",
   "metadata": {},
   "source": [
    "##### Plotting with Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d0644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bin_width= 0.5\n",
    "# # here you can choose your rounding method, I've chosen math.ceil\n",
    "# nbins = math.ceil((df[\"no_of_sentences\"].max() - df[\"no_of_sentences\"].min()) / bin_width)\n",
    "# fig = px.histogram(df, x=\"no_of_sentences\", nbins=nbins,\n",
    "#             width=1200, height=600,\n",
    "#             labels={ # replaces default labels by column name\n",
    "#                 \"no_of_sentences\": \"Number of Sentences\",\n",
    "#             },\n",
    "#             template=\"plotly_white\")\n",
    "# fig.update_yaxes(showgrid=True)\n",
    "# fig.update_layout(title_text=\"Histogram of Number of Sentences in Each Document\", title_x=0.5, font_size=18)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b430ec",
   "metadata": {},
   "source": [
    "##### Plotting with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8219a967",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "sns.histplot(data=df, x=\"no_of_sentences\", bins=100)\n",
    "plt.xlabel('Number of sentences', fontsize=25)\n",
    "plt.ylabel('Count', fontsize=25)\n",
    "plt.title('Histogram of number of sentences in 30000 unlabeled texts', fontsize=25)\n",
    "plt.savefig('histogram of sentences with 100 bins.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9e1c7c",
   "metadata": {},
   "source": [
    "##### Plotting with Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998081ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(20, 10))\n",
    "# plt.hist(df['no_of_sentences'], bins=100, color='royalblue')\n",
    "# plt.title('Histogram of number of sentences in 30000 unlabeled texts', fontsize=25)\n",
    "# plt.xlabel('Number of sentences', fontsize=25)\n",
    "# plt.ylabel('Count', fontsize=25)\n",
    "# # plt.savefig('histogram of sentences with 100 bins.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb3164c",
   "metadata": {},
   "source": [
    "# Phase 3.2: Sentence-wise preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42c329",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8764141",
   "metadata": {},
   "source": [
    "POS\tDESCRIPTION\tEXAMPLES\n",
    "ADJ\tadjective\t*big, old, green, incomprehensible, first*\n",
    "ADP\tadposition\t*in, to, during*\n",
    "ADV\tadverb\t*very, tomorrow, down, where, there*\n",
    "AUX\tauxiliary\t*is, has (done), will (do), should (do)*\n",
    "CONJ\tconjunction\t*and, or, but*\n",
    "CCONJ\tcoordinating conjunction\t*and, or, but*\n",
    "DET\tdeterminer\t*a, an, the*\n",
    "INTJ\tinterjection\t*psst, ouch, bravo, hello*\n",
    "NOUN\tnoun\t*girl, cat, tree, air, beauty*\n",
    "NUM\tnumeral\t*1, 2017, one, seventy-seven, IV, MMXIV*\n",
    "PART\tparticle\t*‚Äôs, not,*\n",
    "PRON\tpronoun\t*I, you, he, she, myself, themselves, somebody*\n",
    "PROPN\tproper noun\t*Mary, John, London, NATO, HBO*\n",
    "PUNCT\tpunctuation\t*., (, ), ?*\n",
    "SCONJ\tsubordinating conjunction\t*if, while, that*\n",
    "SYM\tsymbol\t*$, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù*\n",
    "VERB\tverb\t*run, runs, running, eat, ate, eating*\n",
    "X\tother\t*sfpksdpsxmsa*\n",
    "SPACE\tspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4f3df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(txt):\n",
    "#     dirty_tokens = re.split(' +', txt)  # split words\n",
    "#     # remove all non-alphanumerics\n",
    "#     clean_tokens = [re.sub(r'\\W', '', t).lower() \n",
    "#                     for t in dirty_tokens]\n",
    "#     if '' in clean_tokens:  # remove empty tokens\n",
    "#         clean_tokens.remove('')\n",
    "#     return clean_tokens\n",
    "\n",
    "\n",
    "# def tokenize_spans(spans):\n",
    "#     for s in spans:\n",
    "#         s['tokens_manual'] = tokenize(s['txt'])\n",
    "        \n",
    "        \n",
    "# def build_vocabulary(spans):\n",
    "#     vocab_counts = {}\n",
    "#     for sd in spans:\n",
    "#         for t in tokenize(sd['txt']):\n",
    "#             if t in vocab_counts:\n",
    "#                 vocab_counts[t] += 1\n",
    "#             else:\n",
    "#                 vocab_counts[t] = 1\n",
    "#     return vocab_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2de9a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spacy_tokenize(txt):\n",
    "#     doc = nlp(txt)\n",
    "#     tokens = list(doc)\n",
    "#     clean_tokens = []\n",
    "#     for t in tokens:\n",
    "# #         print(f'{t.text} | {t.lemma_} | {t.pos_}')\n",
    "#         if t.pos_ == 'PUNCT':\n",
    "# #             print(t)\n",
    "#             pass\n",
    "# #         if t.pos_ == 'PART' and re.search(r'\\'', t.text):\n",
    "# #             print(f\"Special case: {t}\")\n",
    "#         elif t.pos_ == 'NUM':\n",
    "#             clean_tokens.append(f'<NUM{len(t)}>')\n",
    "#         else:\n",
    "#             clean_tokens.append(t.lemma_.lower())\n",
    "# #     if ',' in clean_tokens:\n",
    "# #         clean_tokens.remove()\n",
    "#     return clean_tokens\n",
    "\n",
    "# def spans_add_spacy_tokens(spans):\n",
    "#     for s in spans:\n",
    "#         s['tokens_spacy'] = spacy_tokenize(s['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7170f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_spacy_tokenize_v2(txt):\n",
    "#     nlp.disable_pipes('parser')\n",
    "#     doc = nlp.pipe(txt, n_process=4)\n",
    "#     doc = nlp(txt)\n",
    "#     tokens = list(doc)\n",
    "#     clean_tokens = []\n",
    "#     for i in range(len(tokens)):\n",
    "        \n",
    "#         t=tokens[i]\n",
    "#         t1=tokens[i]\n",
    "# #         print(f'{t.text} | {t.lemma_} | {t.pos_}')\n",
    "# #         print(t.pos_, t.text)\n",
    "\n",
    "# #         if \"-\" in t.text:\n",
    "# #             splitted_t = t.text.split(\"-\")\n",
    "# #             for i in range(len(splitted_t)):\n",
    "# #                 clean_tokens.append(splitted_t[i])\n",
    "            \n",
    "#         if(i!=len(tokens)-1):\n",
    "#             t1=tokens[i+1]\n",
    "#         if(t1!=t and t1.pos_=='PART' and re.search(r'\\'', t1.text)):\n",
    "# #             print(t, t1)\n",
    "#             scrap = t.text+t1.text\n",
    "#             scrap = re.sub(r'\\W','',scrap).lower()\n",
    "#             clean_tokens.append(scrap)\n",
    "#             i=i+1           \n",
    "#         elif t.pos_ == 'PUNCT':\n",
    "#             pass\n",
    "#         elif t.text in ('Vet. App.','Fed. Cir.'):\n",
    "#             lem=t.lemma_\n",
    "#             lem=lem.lower()\n",
    "#             clean_tokens.append(lem)\n",
    "#         elif (t.text[0].isalpha()==False and t.is_digit==False):\n",
    "#             if(t.is_upper==False):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 lem=t.lemma_\n",
    "#                 lem=lem.lower()\n",
    "#                 clean_tokens.append(lem)            \n",
    "#         elif t.pos_ == 'NUM':\n",
    "#             clean_tokens.append(f'<NUM{len(t)}>')\n",
    "#         else:\n",
    "#             lem=t.lemma_\n",
    "#             lem = re.sub(r'\\W','',lem)\n",
    "#             lem=lem.lower()\n",
    "#             clean_tokens.append(lem)\n",
    "            \n",
    "#     return clean_tokens\n",
    "\n",
    "# def spans_add_spacy_tokens(spans):\n",
    "#     for s in spans:\n",
    "#         s['tokens_spacy'] = spacy_tokenize(s['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cad40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_spacy_tokenize(txt, nlp):\n",
    "    nlp.disable_pipes('parser')\n",
    "    doc = nlp.pipe(txt, n_process=-1)\n",
    "    doc = nlp(txt)\n",
    "    tokens = list(doc)\n",
    "    clean_tokens = []\n",
    "    \n",
    "#     print(txt)\n",
    "    for i, token in enumerate(tokens):\n",
    "#         print(token)\n",
    "#         print(f\"token: {token} | {token.pos_}\")\n",
    "        if token.pos_ == 'PUNCT' and not re.search(\"^[0-9]{2}/[0-9]{2}/([0-9]{2}|[0-9]{4})$\", token.text):\n",
    "            pass\n",
    "        \n",
    "        elif token.pos_ == 'NUM':\n",
    "            refined_token = re.sub(r'\\W', '', token.text)\n",
    "            clean_tokens.append(f'<NUM{len(refined_token)}>')\n",
    "            \n",
    "        elif token.text == \"\\'s\" and token.pos_ == 'PART':\n",
    "#             print(clean_tokens)\n",
    "#             print(token, tokens[i-1])\n",
    "            pos_token = tokens[i-1].text + token.text\n",
    "            clean_tokens.pop(len(clean_tokens)-1)\n",
    "            clean_tokens.append(pos_token.lower())\n",
    "#             clean_tokens.remove(tokens[i-1].text.lower())\n",
    "            \n",
    "#             refined_token = re.sub(r'\\W', '', pos_token.lower())\n",
    "            \n",
    "#             if refined_token.isnumeric():\n",
    "#                 refined_token = f'<NUM{len(refined_token)}>'\n",
    "                \n",
    "#             if not refined_token == \"\":\n",
    "#                 print(f\"refined token = {refined_token}\")\n",
    "#                 clean_tokens.append(pos_token.lower())\n",
    "#                 clean_tokens.remove(tokens[i-1].text.lower())\n",
    "                   \n",
    "        elif \"-\" in token.text:\n",
    "            splitted_tokens = token.text.split(\"-\")\n",
    "\n",
    "            for sp_token in splitted_tokens:\n",
    "                refined_token = re.sub(r'\\W', '', sp_token.lower())\n",
    "                if refined_token != \"\":\n",
    "                    if refined_token.isnumeric():\n",
    "                        refined_token = f'<NUM{len(refined_token)}>'\n",
    "                    clean_tokens.append(refined_token)\n",
    "        elif token.text in (\"Vet. App.\", \"Fed. Cir.\"):\n",
    "            clean_tokens.append(token.lemma_.lower())\n",
    "#         elif \".\" in token.text and token.text in (\"Vet. App.\", \"Fed. Cir.\"):\n",
    "#             print(f\"Special condition fulfilled: {token.text}\")\n",
    "#             splitted_tokens = token.text.split(\".\")\n",
    "\n",
    "#             for sp_token in splitted_tokens:\n",
    "#                 refined_token = re.sub(r'\\W', '', sp_token.lower())\n",
    "#                 if refined_token != \"\":\n",
    "#                     if refined_token.isnumeric():\n",
    "#                         refined_token = f'<NUM{len(refined_token)}>'\n",
    "#                     clean_tokens.append(refined_token)\n",
    "        else:\n",
    "            refined_token = re.sub(r'\\W', '', token.lemma_.lower())\n",
    "#             print(f\"refined token: {refined_token}\")\n",
    "            if re.search('\\d+', refined_token) and re.search('[a-zA-Z]+', refined_token):\n",
    "#                 print(\"Matched mixed\")\n",
    "                continue\n",
    "            elif refined_token != \"\" and refined_token.isnumeric():\n",
    "#                 print(\"Matched digit only\")\n",
    "                refined_token = f'<NUM{len(refined_token)}>'\n",
    "                clean_tokens.append(refined_token)\n",
    "\n",
    "            elif refined_token != \"\":\n",
    "#                 print(f\"Nothing matched, inserting {refined_token}\")\n",
    "                clean_tokens.append(refined_token)\n",
    "                    \n",
    "    return clean_tokens\n",
    "\n",
    "def custom_spans_add_spacy_tokens(spans, nlp):\n",
    "    for s in spans:\n",
    "        s['tokens_spacy'] = custom_spacy_tokenize(s['txt'], nlp)\n",
    "        s['token_count'] = len(s['tokens_spacy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f409e",
   "metadata": {},
   "source": [
    "#### Testing Spacy tokenizer on some example sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c63db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "# nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55cac51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_basic_1 = 'In sum, as the preponderance of the evidence is against the Veteran\\'s claim, his appeal must be denied.'\n",
    "# example_cit_1 = 'Smith v. Gober, 14 Vet. App. 227 (2000), aff\\'d 281 F.3d 1384 (Fed. Cir. 2002); Dela Cruz v. Principi, 15 Vet. App. 143 (2001); see also Quartuccio v. Principi, 16 Vet. App. 183 (2002).'\n",
    "# example_rule_1 = '\"To establish a right to compensation for a present disability, a Veteran must show: \"(1) the existence of a present disability; (2) in-service incurrence or aggravation of a disease or injury; and (3) a causal relationship between the present disability and the disease or injury incurred or aggravated during service\"-the so-called \"nexus\" requirement.\"'\n",
    "# example_mixed_1 = 'In Dingess v. Nicholson, 19 Vet. App. 473 (2006), the U.S. Court of Appeals for Veterans Claims held that, upon receipt of an application for a service-connection claim, 38 U.S.C.A. ÔøΩ 5103(a) and 38 C.F.R. ÔøΩ 3.159(b) require VA to provide the claimant with notice that a disability rating and an effective date for the award of benefits will be assigned if service connection is awarded. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23386ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_text = \"Veteran's appeal with recurrent tonic/clonic episodes in 1989.\"\n",
    "\n",
    "# print(sp_text, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c2d790",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sp_text_9 = \"1960's with recurrent tonic/clonic episodes in 1989.\"\n",
    "\n",
    "# print(sp_text_9, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text_9 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text_9, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text_9), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89018419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_text_10 = \"1960's with recurrent tonic/clonic episodes in 1989's.\"\n",
    "\n",
    "# print(sp_text_10, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text_10 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text_10, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text_10), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef16e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(example_basic_1, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(example_basic_1 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(example_basic_1, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(example_basic_1), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(example_cit_1, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(example_cit_1 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(example_cit_1, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(example_cit_1), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b910dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(example_rule_1, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(example_rule_1 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(example_rule_1, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(example_rule_1), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e966bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(example_mixed_1, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(example_mixed_1 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(example_mixed_1, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(example_mixed_1), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15041dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_text_1 = \"38 C.F.R. ¬ß 3.303(d) (2005).\"\n",
    "\n",
    "# print(sp_text_1, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text_1 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text_1, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text_1), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0da8502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_text_2 = \"BOARD OF VETERANS APPEALS\\nDEPARTMENT OF VETERANS AFFAIRS\\nWASHINGTON, DC  20420\"\n",
    "\n",
    "# print(sp_text_2, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text_2 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text_2, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text_2), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561fb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_text_3 = \"X-rays demonstrated \\nminimal degenerative changes at the L5-S1.\"\n",
    "\n",
    "# print(sp_text_3, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text_3 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text_3, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text_3), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ce5a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_txt_4 = \"Spacy's tokenizer requires special rules for producing 'state-of-the-art' tokens.\"\n",
    "\n",
    "# print(sp_txt_4, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_txt_4 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_txt_4, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_txt_4), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_txt_5 = \"C.F.R. ¬ß¬ß 3.102, 3.303, 3.306, 3.310 (2005).\"\n",
    "\n",
    "# print(sp_txt_5, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_txt_5 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_txt_5, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_txt_5), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8980be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_text_6 = \"Decision Date: 03/08/06\"\n",
    "\n",
    "# print(sp_text_6, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text_6 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text_6, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text_6), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a03cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_text_7 = \"Archive Date: 06/16/06\"\n",
    "\n",
    "# print(sp_text_7, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text_7 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text_7, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text_7), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6aeca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp_text_8 = 'Significantly, neither the veteran nor his representative has \\nidentified, and the record does not otherwise indicate, any \\nexisting pertinent evidence that has not been obtained.'\n",
    "\n",
    "# print(sp_text_8, end=\"\\n=============\\n\")\n",
    "# print(spacy_tokenize(sp_text_8 ), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize(sp_text_8, nlp), end=\"\\n\\n\")\n",
    "# print(custom_spacy_tokenize_v2(sp_text_8), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786dcd8e",
   "metadata": {},
   "source": [
    "### Test the tokenization of a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79c42e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_file = open(UNLABELED_DATA_PATH/ Path('0603946.txt'), 'r').read().encode('latin-1', 'ignore')\n",
    "# test_file = test_file.decode('latin-1').strip()\n",
    "# doc = luima.text2sentences(test_file, offsets=False)\n",
    "\n",
    "# spans = [{'txt': sent} for sent in doc]\n",
    "# spans_add_spacy_tokens(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c74325",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# token_save_file = 'tokenized_sentences_unlabeled.txt'\n",
    "# file = open(token_save_file, 'w')\n",
    "# dir(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797f9d6a",
   "metadata": {},
   "source": [
    "### Tokenize the whole unlabeled corpus and save it in a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59584032",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = [{'txt': sent} for unlab_doc in unlab_segments_by_doc for sent in unlab_doc['sentences']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bea1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])\n",
    "%time custom_spans_add_spacy_tokens(spans, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc338cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '../Data/unlabeled_corpus_segments_with_spacy_tokens.p'\n",
    "with open (file_name, 'wb') as handle:\n",
    "    pickle.dump(spans, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f5355d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (file_name, 'rb') as handle:\n",
    "    span_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6e502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(span_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be3b89d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da41f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_save_file = '../Data/tokenized_sentences_unlabeled_corpus.txt'\n",
    "# file = open(token_save_file, 'w')\n",
    "# random.seed(42)\n",
    "# random.shuffle(spans)\n",
    "\n",
    "# sents_count_above_threshold = 0\n",
    "# # total_sents_count = 0\n",
    "# for span in tqdm(spans):\n",
    "#     if span['token_count'] > 5:\n",
    "#         for pos, token in enumerate(span['tokens_spacy']):\n",
    "#             if pos == 0:\n",
    "#                 file.write(token)\n",
    "#             else:\n",
    "#                 file.write(\" \" + token)\n",
    "#         sents_count_above_threshold += 1\n",
    "        \n",
    "#         file.write(\"\\n\")        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3017ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(spans), sents_count_above_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b683deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([span['token_count'] for span in spans if span['token_count'] > 5])\n",
    "df.columns = ['token_count_per_sentence']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddee0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.histplot(data=df, x='token_count_per_sentence', bins=100)\n",
    "\n",
    "\n",
    "plt.xlabel('Number of sentences', fontsize=25)\n",
    "plt.ylabel('Token count', fontsize=25)\n",
    "plt.title('Histogram of number of tokens in unlabeled segments', fontsize=25)\n",
    "plt.savefig('token_count_per_sentences_histogram.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599cd2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "595e5d67",
   "metadata": {},
   "source": [
    "##### Observations\n",
    "1. Sometimes converting the date into number, sometimes not. \n",
    "\"Decision Date: 03/08/06 | ['decision', 'Date']\", \"Archive Date: 06/16/06 | ['Archive', 'Date', '<NUM8>']\"\n",
    "\n",
    "2. Not removing the '\\n' characters.\n",
    "BOARD OF VETERANS APPEALS\n",
    "DEPARTMENT OF VETERANS AFFAIRS\n",
    "WASHINGTON, DC  20420 | ['BOARD', 'of', 'VETERANS', 'APPEALS', '\\n', 'DEPARTMENT', 'of', 'VETERANS', 'AFFAIRS', '\\n', 'WASHINGTON', 'DC', ' ', '<NUM5>']\n",
    "\n",
    "3. Capitalizations need to be handled and all should be made lower case. \n",
    "\n",
    "4. Should we keep symbols like this? \"¬ß¬ß\"\n",
    "\n",
    "5. \"70. X-rays demonstrated \n",
    "minimal degenerative changes at the L5-S1. | ['X', '-', 'ray', 'demonstrate', '\\n', 'minimal', 'degenerative', 'change', 'at', 'the', 'L5', 'S1']\"\n",
    "\n",
    "Should the L5-S1 things be kept?\n",
    "\n",
    "6. Should Roman numbers be simplified as well? e.g: 97. II. | ['II']\n",
    "\n",
    "7. Numbering not consistent, e.g: 105. 38 C.F.R. ¬ß 3.303(d) (2005). | ['<NUM2>', 'C.F.R.', '¬ß', '3.303(d', '<NUM4>']\n",
    "\n",
    "8. Vet. App. should be a single token unlike the produced, e.g: 127. Gilbert, 1 Vet. App. at 54. | ['Gilbert', '<NUM1>', 'Vet', '.', 'App', 'at', '<NUM2>']\n",
    "\n",
    "9. 133. James L. March | ['James', 'L.', 'March']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
