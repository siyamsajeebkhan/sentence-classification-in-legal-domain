{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53761d7f",
   "metadata": {},
   "source": [
    "##### Question: related to section 2: segmentation\n",
    "Hi,\n",
    "\n",
    "I got relatively high precision and recall on my Improved segmentation analysis (i.e. precision 0.89 and recall 0.98) and I just solved a couple of problems. So, I just want to be sure whether I did things correct.\n",
    "\n",
    "Basically, I iterate over the beginning indexes of annotations and then look whether there is a index in range +-3 in the beginning indexes of segmentations. Is this correct? Currently, my algorithm make ordinal numbers sentences (i.e. 1., 2. etc.) wrong, but since it is just two characters, the next sentence is considered as correct in precision and recall calculations. Should I solve also this ordinal numbers problem?\n",
    "\n",
    "##### Prof's ans:\n",
    "These numbers look realistic to me. The ordinal numbers problem is one example of an observation you should discuss in the report in order to get full credit. You fix it in the code if you like, but it is a minor issue and not necessary as long as you survey and discuss the problem in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384bbe72",
   "metadata": {},
   "source": [
    "##### Question: related to section 5\n",
    "For 5.2 for each sentence is the expectation to get an average of all token's vector representation and use that average vector as a feature, I just wanted to make sure\n",
    "\n",
    "\n",
    "##### Ans:\n",
    "\n",
    "Correct. The average vector's dimensions is the same as your embedding dimension. Adding features to it (e.g., the sentence position) will increase its size accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4847abb8",
   "metadata": {},
   "source": [
    "##### Question:\n",
    "\n",
    "Because in some sentences, there are still \"\\r\\n\" left after the sentence segmentation process.\n",
    "Although they are also existing in the test set also (so it is better to keep them?)\n",
    "\n",
    "\n",
    "A further question is that, in the manual tokenization we were removing all non-alphanumeric characters, however I am confused whether or not we should remove the \".\" in some instances as it implies in step 4.2? Also the \"Vet. App.\" example confused me, if we are supposed to remove all punctuation, or do we need to remove only standalone punctuation?\n",
    "\n",
    "\n",
    "##### Ans:\n",
    "\n",
    "Again, think about what the purpose of the exercise is. ***Removing all punctuation after tokenization makes the data less noisy*** because the last word in a sentence will not have a training period that separates it from the non-sentence-end version in the vocabulary. On the other hand, words like \"app\"/\"app.\" and \"vet\"/\"vet.\" become indistinguishable, which to me seems like only a minor issue. So yes, I would remove periods.\n",
    "\n",
    "***Doing it without discussing it will not give you full credit, though. As the project instruction state, the point is that you explain in the report \"what preprocessing steps do you include, and why do you include them?\"***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaf317a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da141396",
   "metadata": {},
   "source": [
    "### Tips:\n",
    "Do not preprocess anything before the segmenter! If you do, your GLOSS annotation offsets will no longer match as they include whitespace and non-printable characters. So yes, feed it into the segmenter as is. This is a realistic condition as your analysis pipeline should be maximally compatible with the export of your annotation tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e38c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
