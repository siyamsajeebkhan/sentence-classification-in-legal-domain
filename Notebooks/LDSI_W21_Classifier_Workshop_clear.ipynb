{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf4257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r\"C:\\Users\\siyam\\OneDrive\\Desktop\\Legal DS\\project\\venv\\Lib\\site-packages\")\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a01323",
   "metadata": {},
   "source": [
    "## Some boilerplate code for nicer looking confusion matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16b76f",
   "metadata": {},
   "source": [
    "abridged from [scikit-learn example code](https://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ec052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fde4cb5",
   "metadata": {},
   "source": [
    "## Some helper code to analyze TFIDF data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09239f14",
   "metadata": {},
   "source": [
    "Credit for code goes to [buhrmann.github.io](https://buhrmann.github.io/tfidf-analysis.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3df1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=15):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_features_in_doc(Xtr, features, row_id, top_n=15):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    xtr_row = Xtr[row_id]\n",
    "    if type(xtr_row) is not np.ndarray:\n",
    "        xtr_row = xtr_row.toarray()\n",
    "    row = np.squeeze(xtr_row)\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "\n",
    "def top_mean_features(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]\n",
    "    else:\n",
    "        D = Xtr\n",
    "    if type(D) is not np.ndarray:\n",
    "        D = D.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "def top_features_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def span_top_tfidf(spans_txt, spans_tfidf, features, index):\n",
    "    print('span text:\\n'+spans_txt[index]+'\\n')\n",
    "    print(top_features_in_doc(spans_tfidf, features, index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51593f7",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb485676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_fpath = '../annotation_data/ldsi_w21_final_annotations_v1.json'\n",
    "corpus_fpath = \"../Data/ldsi_w2021-20220221T223611Z-001/ldsi_w2021/ldsi_w21_curated_annotations_v2.json\"\n",
    "data = json.load(open(corpus_fpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4858862",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3800461",
   "metadata": {},
   "source": [
    "Define some convenience shorthands and dictionaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1456a587",
   "metadata": {},
   "source": [
    "# Data Structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c649d",
   "metadata": {},
   "source": [
    "Examine data structures so we know how to work with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d883969",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf7084",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(types_by_id.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5164c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(documents_by_id.items())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86300e3",
   "metadata": {},
   "source": [
    "# Very Basic Data Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba57567",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(documents_by_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b496c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lengths = [len(d['plainText']) for d in documents_by_id.values()]\n",
    "plt.hist(doc_lengths, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada34f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_num_annos = [len([a for a in annotations if a['document'] == doc_id])\n",
    "                 for doc_id in documents_by_id]\n",
    "doc_num_annos = [n for n in doc_num_annos if n > 0]\n",
    "plt.hist(doc_num_annos, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb442817",
   "metadata": {},
   "source": [
    "# Create Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29062ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sentences assuming every annotation is a sentence\n",
    "def make_span_data(documents_by_id, types_by_id, annotations):\n",
    "    span_data = []\n",
    "    for a in annotations:\n",
    "        start = a['start']\n",
    "        end = a['end']\n",
    "        document_txt = documents_by_id[a['document']]['plainText']\n",
    "        atype = a['type']\n",
    "        sd = {'txt': document_txt[start:end],\n",
    "              'document': a['document'],\n",
    "              'type': types_by_id[atype]['name'],\n",
    "              'start': a['start'],\n",
    "              'start_normalized': a['start'] / len(document_txt),\n",
    "              'end': a['end']}\n",
    "        span_data.append(sd)\n",
    "    return span_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dc0b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = make_span_data(documents_by_id, types_by_id, annotations)\n",
    "span_labels = [s['type'] for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc991ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5ffe6",
   "metadata": {},
   "source": [
    "Sample train and test set while stratifying across types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spans, test_spans = model_selection.train_test_split(spans,\n",
    "                                                           test_size=.2,\n",
    "                                                           random_state=42,\n",
    "                                                           stratify=span_labels)\n",
    "train_spans_txt = [s['txt'] for s in train_spans]\n",
    "test_spans_txt = [s['txt'] for s in test_spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_spans))\n",
    "print(len(test_spans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8091754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e08e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spans_txt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fb1db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train: {len(train_spans)}; test: {len(test_spans)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cec863",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(train_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11acedd9",
   "metadata": {},
   "source": [
    "# Some Tough Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2467fb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_basic_1 = 'In sum, as the preponderance of the evidence is against the Veteran\\'s claim, his appeal must be denied.'\n",
    "example_cit_1 = 'Smith v. Gober, 14 Vet. App. 227 (2000), aff\\'d 281 F.3d 1384 (Fed. Cir. 2002); Dela Cruz v. Principi, 15 Vet. App. 143 (2001); see also Quartuccio v. Principi, 16 Vet. App. 183 (2002).'\n",
    "example_rule_1 = '\"To establish a right to compensation for a present disability, a Veteran must show: \"(1) the existence of a present disability; (2) in-service incurrence or aggravation of a disease or injury; and (3) a causal relationship between the present disability and the disease or injury incurred or aggravated during service\"-the so-called \"nexus\" requirement.\"'\n",
    "example_mixed_1 = 'In Dingess v. Nicholson, 19 Vet. App. 473 (2006), the U.S. Court of Appeals for Veterans Claims held that, upon receipt of an application for a service-connection claim, 38 U.S.C.A. � 5103(a) and 38 C.F.R. � 3.159(b) require VA to provide the claimant with notice that a disability rating and an effective date for the award of benefits will be assigned if service connection is awarded. '"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9467688",
   "metadata": {},
   "source": [
    "# Manual Tokenization Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    dirty_tokens = re.split(' +', txt)  # split words\n",
    "    # remove all non-alphanumerics\n",
    "    clean_tokens = [re.sub(r'\\W', '', t).lower() \n",
    "                    for t in dirty_tokens]\n",
    "    if '' in clean_tokens:  # remove empty tokens\n",
    "        clean_tokens.remove('')\n",
    "    return clean_tokens\n",
    "\n",
    "\n",
    "def tokenize_spans(spans):\n",
    "    for s in spans:\n",
    "        s['tokens_manual'] = tokenize(s['txt'])\n",
    "        \n",
    "        \n",
    "def build_vocabulary(spans):\n",
    "    vocab_counts = {}\n",
    "    for sd in spans:\n",
    "        for t in tokenize(sd['txt']):\n",
    "            if t in vocab_counts:\n",
    "                vocab_counts[t] += 1\n",
    "            else:\n",
    "                vocab_counts[t] = 1\n",
    "    return vocab_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b86371",
   "metadata": {},
   "source": [
    "We can use this basic tokenizer to do some surface statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d74d0b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenize(example_cit_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e2ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_spans(train_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_counts_manual = build_vocabulary(train_spans)\n",
    "unique_tokens_manual = [token for token, count in vocab_counts_manual.items() \n",
    "                        if count == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d475b078",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vocab_counts_manual))\n",
    "print(vocab_counts_manual['veteran'])\n",
    "print(len(unique_tokens_manual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b889baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_lengths_manual = [len(s['tokens_manual']) for s in train_spans]\n",
    "plt.hist(span_lengths_manual, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a717e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_df = pd.DataFrame([{'token': t, 'count': c} for t, c in vocab_counts_manual.items()])\n",
    "vocab_df = vocab_df.set_index(['token'])\n",
    "vocab_df.sort_values('count', ascending=False)[:30].plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2881c4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_freq = 6\n",
    "max_freq = 100\n",
    "feature_names_manual = sorted(token for token, count in vocab_counts_manual.items() \n",
    "                       if min_freq <= count <= max_freq)\n",
    "print(f'number of thresholded with {min_freq} < n < {max_freq}: {len(feature_names_manual)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98dd15e",
   "metadata": {},
   "source": [
    "# Basic TFIDF vectorization with sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3b650d",
   "metadata": {},
   "source": [
    "We are using sklear's [TFIDF vectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer.fit) here, because it is faster than doing it all by ourselves and has useful default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae29f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=5)\n",
    "vectorizer = vectorizer.fit(train_spans_txt)\n",
    "tfidf_features_skl = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_skl = vectorizer.transform(train_spans_txt).toarray()\n",
    "test_tfidf_skl = vectorizer.transform(test_spans_txt).toarray()\n",
    "train_spans_labels = np.array([s['type'] for s in train_spans])\n",
    "test_spans_labels = np.array([s['type'] for s in test_spans])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471ed8d",
   "metadata": {},
   "source": [
    "... and we get numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7e38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tfidf_skl.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbe794a",
   "metadata": {},
   "source": [
    "Examine the top TFIDF values of tokens in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1246e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "span_top_tfidf(train_spans_txt, \n",
    "               train_tfidf_skl,\n",
    "               tfidf_features_skl,\n",
    "               random.randint(0, len(train_spans)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769f749c",
   "metadata": {},
   "source": [
    "Examine features with highest average TFIDF score per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040105a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_features_by_class(train_tfidf_skl, \n",
    "                            train_spans_labels,\n",
    "                            tfidf_features_skl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ce4952",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Citation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f653b0cc",
   "metadata": {},
   "source": [
    "## Train & Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a0bbd",
   "metadata": {},
   "source": [
    "Models:  \n",
    "[Gaussian Naive Bayes](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)  \n",
    "[Support Vector Machine Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)  \n",
    "[Decision Tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)  \n",
    "[Random Forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5ec1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = SVC(gamma='auto')\n",
    "#clf = RandomForestClassifier(n_estimators=100, max_depth=12)\n",
    "#clf = tree.DecisionTreeClassifier(max_depth=8)\n",
    "clf_skl = GaussianNB()\n",
    "clf_skl = clf_skl.fit(train_tfidf_skl, train_spans_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c61ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN:\\n'+classification_report(train_spans_labels, \n",
    "                                       clf_skl.predict(train_tfidf_skl)))\n",
    "print('TEST:\\n'+classification_report(test_spans_labels,\n",
    "                                      clf_skl.predict(test_tfidf_skl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607aa8a6",
   "metadata": {},
   "source": [
    "# Build Feature Vector using Spacy\n",
    "More information: [Spacy](https://spacy.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527b00cc",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28285c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic English pipeline provided by spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea6da5b",
   "metadata": {},
   "source": [
    "More information on customizing tokenizers [at the Spacy docs](https://spacy.io/usage/linguistic-features#tokenization)  \n",
    "Note that you may not alter the text, but you can use it to suppress or force tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041157cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25dcd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_txt = example_basic_1\n",
    "#test_txt = example_cit_1\n",
    "test_txt = example_mixed_1\n",
    "print(test_txt)\n",
    "doc = nlp(test_txt)\n",
    "print(f'{len(list(doc.sents))} sentences:')\n",
    "print('===')\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "print('===')\n",
    "for t in doc:\n",
    "    print(f'{t.text} | {t.lemma_} | {t.pos_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85ae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(txt):\n",
    "    doc = nlp(txt)\n",
    "    tokens = list(doc)\n",
    "    clean_tokens = []\n",
    "    for t in tokens:\n",
    "        if t.pos_ == 'PUNCT':\n",
    "            pass\n",
    "        elif t.pos_ == 'NUM':\n",
    "            clean_tokens.append(f'<NUM{len(t)}>')\n",
    "        else:\n",
    "            clean_tokens.append(t.lemma_)\n",
    "    return clean_tokens\n",
    "\n",
    "def spans_add_spacy_tokens(spans):\n",
    "    for s in spans:\n",
    "        s['tokens_spacy'] = spacy_tokenize(s['txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55843c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_spans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d7fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# will take a moment\n",
    "spans_add_spacy_tokens(train_spans)\n",
    "spans_add_spacy_tokens(test_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2298137",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choice(train_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0091754",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87eec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# suboptimal: tokenizer gets called twice\n",
    "spacy_tfidf_vectorizer = TfidfVectorizer(tokenizer=spacy_tokenize,\n",
    "                                         min_df=3,\n",
    "                                         ngram_range=(1,1))\n",
    "spacy_tfidf_vectorizer = spacy_tfidf_vectorizer.fit(train_spans_txt)\n",
    "tfidf_features_spacy = spacy_tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498d032d",
   "metadata": {},
   "source": [
    "### Small walkthrough of how to extend/compose feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f06f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy feature vector extension\n",
    "train_tfidf_spacy = spacy_tfidf_vectorizer.transform(train_spans_txt).toarray()\n",
    "print(train_tfidf_spacy.shape)\n",
    "train_starts_normalized = np.array([s['start_normalized'] for s in train_spans])\n",
    "print(train_starts_normalized.shape)\n",
    "print(np.expand_dims(train_starts_normalized, axis=1).shape)\n",
    "ext = np.concatenate((train_tfidf_spacy, \n",
    "                      np.expand_dims(train_starts_normalized, axis=1)), axis=1)\n",
    "print(ext.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01528477",
   "metadata": {},
   "source": [
    "### Examine highest average TFIDF features by class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = top_features_by_class(train_tfidf_spacy, train_spans_labels, tfidf_features_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs['Citation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a34b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_feature_vectors_and_labels(spans, vectorizer):\n",
    "    # function takes long to execute\n",
    "    # note: we un-sparse the matrix here to be able to manipulate it\n",
    "    tfidf = spacy_tfidf_vectorizer.transform([s['txt'] for s in spans]).toarray()\n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    num_tokens = np.array([len(s['tokens_spacy']) for s in spans])\n",
    "    y = np.array([s['type'] for s in spans])\n",
    "    X = np.concatenate((tfidf, np.expand_dims(starts_normalized, axis=1)), axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17317ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_y = make_feature_vectors_and_labels(train_spans, spacy_tfidf_vectorizer)\n",
    "test_X, test_y = make_feature_vectors_and_labels(test_spans, spacy_tfidf_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc3bee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{train_X.shape} {train_y.shape}')\n",
    "print(f'{test_X.shape} {test_y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = GaussianNB()\n",
    "clf = tree.DecisionTreeClassifier(max_depth=12)\n",
    "clf = clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60868d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN:\\n'+classification_report(train_spans_labels, clf.predict(train_X)))\n",
    "print('TEST:\\n'+classification_report(test_spans_labels, clf.predict(test_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f77969",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_spans_labels, clf.predict(test_X), classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498b0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_names_by_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63160b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_errors(clf, eval_spans, vectorizer, \n",
    "                      select_true_label=None, \n",
    "                      select_pred_label=None):\n",
    "    eval_X, eval_y = make_feature_vectors_and_labels(eval_spans, vectorizer)\n",
    "    eval_spans_txt = [s['txt'] for s in eval_spans]\n",
    "    eval_spans_labels = [s['type'] for s in eval_spans]\n",
    "    pred_y = clf.predict(eval_X)\n",
    "    for i in range(len(eval_spans)):\n",
    "        true_label = eval_spans_labels[i]\n",
    "        pred_label = pred_y[i]\n",
    "        if true_label != pred_label:\n",
    "            if select_true_label and true_label != select_true_label: continue\n",
    "            if select_pred_label and pred_label != select_pred_label: continue\n",
    "            doc_name = documents_by_id[eval_spans[i]['document']]['name']\n",
    "            print('sentence # '+str(i)+' / case '+doc_name+' / @'+str(eval_spans[i]['start']))\n",
    "            print('pred: '+pred_label+' / true: '+true_label)\n",
    "            print(eval_spans[i]['txt'])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5607c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_errors(clf,\n",
    "                  random.sample(train_spans, 100),\n",
    "                  spacy_tfidf_vectorizer,\n",
    "                  select_pred_label='Evidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a537f2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
