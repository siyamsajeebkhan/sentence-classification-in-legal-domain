{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119f3abd-f42a-46b3-bfd6-7e2fc4b946b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.language import Language\n",
    "from luima_sbd import sbd_utils as luima"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792f7a07",
   "metadata": {},
   "source": [
    "# Phase 2 - Decisidng on a Sentence Segmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5055786",
   "metadata": {},
   "source": [
    "### Defining some functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74760ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_word_headers = [\"REPRESENTATION\",\n",
    "                       \"____________________________________________\",\n",
    "                       \"ORDER\",\n",
    "                       \"INTRODUCTION\",\n",
    "                      ]\n",
    "\n",
    "\n",
    "other_headers = [\"THE ISSUE\",\n",
    "                 \"WITNESS AT HEARING ON APPEAL\",\n",
    "                 \"ATTORNEY FOR THE BOARD\",\n",
    "                 \"FINDINGS OF FACT\",\n",
    "                 \"CONCLUSION OF LAW\",\n",
    "                 \"REASONS AND BASES FOR FINDING AND CONCLUSION\",\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059e5a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sentences assuming every annotation is a sentence\n",
    "def make_span_data(documents_by_id, types_by_id, annotations):\n",
    "    span_data = []\n",
    "    for a in annotations:\n",
    "        start = a['start']\n",
    "        end = a['end']\n",
    "        document_txt = documents_by_id[a['document']]['plainText']\n",
    "        atype = a['type']\n",
    "        sd = {'txt': document_txt[start:end],\n",
    "              'document': a['document'],\n",
    "              'type': types_by_id[atype]['name'],\n",
    "              'start': a['start'],\n",
    "              'start_normalized': a['start'] / len(document_txt),\n",
    "              'end': a['end']}\n",
    "        span_data.append(sd)\n",
    "    return span_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9bc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_ann_span_by_doc(spans, doc_id):\n",
    "    start = []\n",
    "    end = []\n",
    "    for span in spans:\n",
    "        if span['document'] == doc_id:\n",
    "            start.append(span['start'])\n",
    "            end.append(span['end'])\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0592337",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ann_span_by_doc_with_spacy(train_doc_ids, nlp):\n",
    "    gen_ann_span_by_doc = {}\n",
    "    for train_id in tqdm(train_doc_ids, disable=True):\n",
    "        text = documents_by_id[train_id]['plainText']\n",
    "        doc = nlp(text)\n",
    "        sentences = list(doc.sents)\n",
    "        sentence_starts = [sent.start_char for sent in sentences]\n",
    "        sentence_ends = [sent.end_char for sent in sentences]\n",
    "        gen_ann_span_by_doc[train_id] = {'start': sentence_starts, 'end': sentence_ends}\n",
    "    return gen_ann_span_by_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40c94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ann_span_by_doc_with_luima(train_doc_ids, nlp=None):\n",
    "    gen_ann_span_by_doc = {}\n",
    "    for train_id in tqdm(train_doc_ids, disable=True):\n",
    "        text = documents_by_id[train_id]['plainText'].strip()\n",
    "        doc = luima.text2sentences(text, offsets=False)\n",
    "        indices = luima.text2sentences(text, offsets=True)\n",
    "        \n",
    "        sentence_starts = [ind[0] for ind in indices]\n",
    "        sentence_ends = [ind[1] for ind in indices]\n",
    "        gen_ann_span_by_doc[train_id] = {'start': sentence_starts, 'end': sentence_ends}\n",
    "    return gen_ann_span_by_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a1829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_closest_start_point(train_doc_ids, true_ann_span_by_doc, gen_ann_span_by_doc):\n",
    "    closest_by_id = {}\n",
    "    for train_id in train_doc_ids:\n",
    "        true_spans = []\n",
    "        closest_neighbors = []\n",
    "        for true_start, true_end in zip(true_ann_span_by_doc[train_id]['start'], true_ann_span_by_doc[train_id]['end']):\n",
    "            dist = 7000000\n",
    "            for gen_start, gen_end in zip(gen_ann_span_by_doc[train_id]['start'], gen_ann_span_by_doc[train_id]['end']):\n",
    "                cal_dist = abs(true_start - gen_start)\n",
    "                if cal_dist < dist:\n",
    "                    dist = cal_dist\n",
    "                    closest_neighbor = {'start': gen_start, 'end': gen_end}\n",
    "            \n",
    "            true_spans.append({'start': true_start, 'end': true_end})\n",
    "            closest_neighbors.append(closest_neighbor)\n",
    "        closest_by_id[train_id] = {'true': true_spans, 'pred': closest_neighbors}\n",
    "    return closest_by_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b3d8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_error_metrics(train_doc_ids, true_ann_span_by_doc, gen_ann_span_by_doc, closest_by_id):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    \n",
    "    tot_true_splits = 0\n",
    "    tot_gen_splits = 0\n",
    "    for train_id in train_doc_ids:\n",
    "        true_split_len = len(true_ann_span_by_doc[train_id]['start'])\n",
    "        gen_split_len = len(gen_ann_span_by_doc[train_id]['start'])\n",
    "        \n",
    "        tot_true_splits += true_split_len\n",
    "        tot_gen_splits += gen_split_len\n",
    "        \n",
    "        \n",
    "        true_starts = np.array([true_span['start'] for true_span in closest_by_id[train_id]['true']])\n",
    "        pred_closest_starts = np.array([pred_span['start'] for pred_span in closest_by_id[train_id]['pred']])\n",
    "        \n",
    "        tp_doc = ((abs(true_starts - pred_closest_starts))<=3).sum()\n",
    "        fn_doc = true_split_len - tp_doc\n",
    "        fp_doc = gen_split_len - tp_doc\n",
    "        \n",
    "        TP += tp_doc\n",
    "        FP += fp_doc\n",
    "        FN += fn_doc\n",
    "        \n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "#     print(f\"True split length: {tot_true_splits}\")\n",
    "#     print(f\"Generated split length: {tot_gen_splits}\")\n",
    "#     print(f\"TP: {TP}, FP: {FP}, FN: {FN}\\n\")\n",
    "    return tot_true_splits, tot_gen_splits, TP, FP, FN, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff0beee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_segmenter(train_doc_ids, segmenter, nlp=None):\n",
    "    if segmenter == 'luima':\n",
    "        ann_span_generator = generate_ann_span_by_doc_with_luima\n",
    "    elif segmenter == 'spacy':\n",
    "        ann_span_generator = generate_ann_span_by_doc_with_spacy\n",
    "    error_metrics = []\n",
    "    for doc_id in tqdm(train_doc_ids):\n",
    "        gen_ann_span = ann_span_generator([doc_id], nlp)\n",
    "        closest_by_id = find_closest_start_point(\n",
    "            [doc_id], \n",
    "            true_ann_span_by_doc, \n",
    "            gen_ann_span\n",
    "        )\n",
    "\n",
    "\n",
    "        total_true_splits, total_gen_splits, tp, fp, fn, precision, recall, f1_score = calculate_error_metrics(\n",
    "            [doc_id], \n",
    "            true_ann_span_by_doc, \n",
    "            gen_ann_span, \n",
    "            closest_by_id\n",
    "        )\n",
    "\n",
    "        em_doc = {\n",
    "            'doc_id': doc_id,\n",
    "            'true_split_count': total_true_splits,\n",
    "            'gen_split_count': total_gen_splits,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'precision': round(precision, 2),\n",
    "            'recall': round(recall, 2),\n",
    "            'f1_score': round(f1_score, 2)\n",
    "        }\n",
    "        error_metrics.append(em_doc)\n",
    "    return error_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04337d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_error_metrics(error_metrics, segmenter):\n",
    "    TP = sum([em['tp'] for em in error_metrics])\n",
    "    FP = sum([em['fp'] for em in error_metrics])\n",
    "    FN = sum([em['fn'] for em in error_metrics])\n",
    "\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    # print(f\"True split length: {tot_true_splits}\")\n",
    "    # print(f\"Generated split length: {tot_gen_splits}\")\n",
    "    # print(f\"TP: {TP}, FP: {FP}, FN: {FN}\\n\")\n",
    "\n",
    "    print(f\"Error metrics using the {segmenter} segmenter:\")\n",
    "    print(f'Precision: {precision:.2f}\\nRecall: {recall:.2f}\\nF1_score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28b8878",
   "metadata": {},
   "source": [
    "##### EXTEND THE SPACY'S STANDARD SEGMENTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abacb4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXTEND THE SPACY'S STANDARD SEGMENTER\n",
    "@Language.component(\"set_custom_boundaries\")\n",
    "def set_custom_boundaries(doc):\n",
    "    for i in range(len(doc)):\n",
    "        if doc[i].text in (\"’s\", \"'s\"):\n",
    "            doc[i].is_sent_start = False\n",
    "        elif doc[i].text in (\"\\n\", \"\\t\", \"\\r\", \"DC.\",\"Archive\", \"NO.\"):\n",
    "            doc[i].is_sent_start = False    \n",
    "        elif doc[i].text ==\"DOCKET\" and doc[i+1:i+3].text ==\"NO.\":\n",
    "            doc[i].is_sent_start = False\n",
    "            end = i+3\n",
    "            while i+1<=end:\n",
    "                doc[i+1].is_sent_start = False\n",
    "                i += 1\n",
    "        elif doc[i].text in single_word_headers:\n",
    "            doc[i].is_sent_start = True\n",
    "            i += 1\n",
    "            while (doc[i].text.isspace()):\n",
    "                i += 1\n",
    "            doc[i].is_sent_start = True\n",
    "        # Fixed    \n",
    "        elif doc[i].text == \"THE\" and doc[i+1].text == \"ISSUE\":\n",
    "            doc[i].is_sent_start = True\n",
    "            i += 2\n",
    "            while (doc[i].text.isspace()):\n",
    "                i += 1\n",
    "            doc[i].is_sent_start = True\n",
    "        # Fixed\n",
    "        elif doc[i].text == \"WITNESS\" and doc[i+1: i+5].text == \"AT HEARING ON APPEAL\":\n",
    "            doc[i].is_sent_start = True\n",
    "            end = i + 5\n",
    "            while i+1<=end:\n",
    "                doc[i+1].is_sent_start = False\n",
    "                i += 1\n",
    "            while (doc[i].text.isspace()):\n",
    "                i += 1\n",
    "            doc[i].is_sent_start = True\n",
    "        # Fixed    \n",
    "        elif doc[i].text == \"ATTORNEY\" and doc[i+1: i+4].text == \"FOR THE BOARD\":\n",
    "            doc[i].is_sent_start = True\n",
    "            end = i + 4\n",
    "            while i+1 <= end:\n",
    "                doc[i+1].is_sent_start = False\n",
    "                i += 1\n",
    "            while (doc[i].text.isspace()):\n",
    "                i += 1\n",
    "            doc[i].is_sent_start = True            \n",
    "        # Fixed\n",
    "        elif (doc[i].text == \"FINDINGS\" or doc[i].text == \"FINDING\") and doc[i+1: i+3].text == \"OF FACT\":\n",
    "            doc[i].is_sent_start = True\n",
    "            end = i + 3\n",
    "            while i+1 <= end:\n",
    "                doc[i+1].is_sent_start = False\n",
    "                i += 1\n",
    "            while (doc[i].text.isspace()):\n",
    "                i += 1\n",
    "            doc[i].is_sent_start = True      \n",
    "        # Fixed    \n",
    "        elif doc[i].text == \"CONCLUSION\" and doc[i+1: i+3].text == \"OF LAW\":\n",
    "            doc[i].is_sent_start = True\n",
    "            end = i + 3\n",
    "            while i+1 <= end:\n",
    "                doc[i+1].is_sent_start = False\n",
    "                i += 1\n",
    "            while (doc[i].text.isspace()):\n",
    "                i += 1\n",
    "            doc[i].is_sent_start = True  \n",
    "        # Fixed    \n",
    "        elif doc[i].text == \"REASONS\" and (doc[i+1: i+7].text == \"AND BASES FOR FINDING AND CONCLUSION\" or doc[i+1: i+7].text == \"AND BASES FOR FINDINGS AND CONCLUSION\"):\n",
    "            doc[i].is_sent_start = True\n",
    "            end = i + 7\n",
    "            while i+1 <= end:\n",
    "                doc[i+1].is_sent_start = False\n",
    "                i += 1\n",
    "            while (doc[i].text.isspace()):\n",
    "                i += 1\n",
    "            doc[i].is_sent_start = True      \n",
    "        elif doc[i].text.lower() == \"on\" and doc[i+1:i+4].text == \"appeal from the\":\n",
    "            doc[i].is_sent_start = True\n",
    "            end = i + 4\n",
    "            while i+1 <= end:\n",
    "                doc[i+1].is_sent_start = False\n",
    "                i += 1\n",
    "            \n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d9d3a",
   "metadata": {},
   "source": [
    "### Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e88c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURATED_ANN_PATH = \"../Data/ldsi_w21_curated_annotations_v2.json\"\n",
    "with open(CURATED_ANN_PATH, 'r') as j:\n",
    "     data = json.loads(j.read())\n",
    "        \n",
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}\n",
    "\n",
    "\n",
    "granted_doc_ids = set([doc['_id'] for doc in data['documents'] if doc['outcome'] == 'granted'])\n",
    "denied_doc_ids = set([doc['_id'] for doc in data['documents'] if doc['outcome'] == 'denied'])\n",
    "print(len(granted_doc_ids), len(denied_doc_ids))\n",
    "\n",
    "# Filter out the IDs of the 141 documents from a total of 540\n",
    "ids_annotated_docs = set([ann['document'] for ann in data['annotations']])\n",
    "print(len(ids_annotated_docs))\n",
    "\n",
    "\n",
    "granted_ids = sorted(list(granted_doc_ids.intersection(ids_annotated_docs)))\n",
    "denied_ids = sorted(list(denied_doc_ids.intersection(ids_annotated_docs)))\n",
    "print(len(granted_ids), len(denied_ids))\n",
    "\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "random.shuffle(granted_ids)\n",
    "random.shuffle(denied_ids)\n",
    "granted_train, granted_val, granted_test = np.split(granted_ids, [int(len(granted_ids)*0.8), int(len(granted_ids)*0.9)])\n",
    "denied_train, denied_val, denied_test = np.split(denied_ids, [57, 64])\n",
    "\n",
    "\n",
    "train_set, dev_set, test_set = np.concatenate((granted_train, denied_train), axis=0), \\\n",
    "                                np.concatenate((granted_val, denied_val), axis=0), \\\n",
    "                                    np.concatenate((granted_test, denied_test), axis=0), \\\n",
    "\n",
    "print(train_set.shape, dev_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading the training doc ids\n",
    "# train_doc_ids = np.load('../Data/train.npy')\n",
    "# train_doc_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spans = make_span_data(documents_by_id, types_by_id, annotations)\n",
    "span_labels = [s['type'] for s in spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c43651",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating the annotation span for the train data with original spans\n",
    "train_doc_ids = train_set\n",
    "true_ann_span_by_doc = {}\n",
    "for train_id in train_doc_ids:\n",
    "    ann_span_starts, ann_span_ends = prepare_ann_span_by_doc(spans, train_id)\n",
    "    true_ann_span_by_doc[train_id] = {'start': ann_span_starts, 'end': ann_span_ends}\n",
    "\n",
    "len(true_ann_span_by_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b157eab",
   "metadata": {},
   "source": [
    "## Step 2.1: Standard Spacy segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36f36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the error metrics for all training docs with standard saegmenter\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "error_maetrics_spacy_std = analyze_segmenter(train_doc_ids, 'spacy', nlp)\n",
    "print_error_metrics(error_maetrics_spacy_std, 'standard spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203f040f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Three doc ids with lowest precision\n",
    "df_std = pd.DataFrame(error_maetrics_spacy_std)\n",
    "low_f1_doc_ids_df = df_std.sort_values('precision').head(3)\n",
    "\n",
    "low_scoring_docs = []\n",
    "for row in low_f1_doc_ids_df.iterrows():\n",
    "    dic = {\n",
    "        'doc_id': row[1].doc_id,\n",
    "        'precision': row[1].precision,\n",
    "        'recall': row[1].recall,\n",
    "        'f1_score': row[1].f1_score\n",
    "        \n",
    "    }\n",
    "    low_scoring_docs.append(dic)\n",
    "low_scoring_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450254dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_f1_doc_ids_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4aeb2e",
   "metadata": {},
   "source": [
    "## Step 2.2: Improved segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7d709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the error metrics for all training docs with extended saegmenter\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])\n",
    "\n",
    "error_maetrics_spacy_ext = analyze_segmenter(train_doc_ids, 'spacy', nlp)\n",
    "print_error_metrics(error_maetrics_spacy_ext, 'extended spacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0fab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the error metrics\n",
    "df_ext = pd.DataFrame(error_maetrics_spacy_ext)\n",
    "df_ext.sort_values('f1_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493d35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See whether the scores for the three docs with lowest precision have improved\n",
    "# with the extended spacy segmenter\n",
    "\n",
    "for doc in low_scoring_docs:\n",
    "    improved_precision = df_ext[df_ext['doc_id']==doc['doc_id']]['precision'].values[0]\n",
    "    improved_recall = df_ext[df_ext['doc_id']==doc['doc_id']]['recall'].values[0]\n",
    "    improved_f1 = df_ext[df_ext['doc_id']==doc['doc_id']]['f1_score'].values[0]\n",
    "    print(f\"Comparison between standard and improved segmenter for document {doc['doc_id']}:\")\n",
    "    \n",
    "    print(f\"Old Precision: {doc['precision']}, improved precision: {improved_precision}\")\n",
    "    print(f\"Recall: {doc['recall']}, improved_recall: {improved_recall}\")\n",
    "    print(f\"F1 score: {doc['f1_score']}, improved_f1: {improved_f1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132fac61",
   "metadata": {},
   "source": [
    "## Step 2.3: Law-specific sentence segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the error metrics for all training docs with LUIMA saegmenter\n",
    "error_metrics_luima = analyze_segmenter(train_doc_ids, 'luima')\n",
    "print_error_metrics(error_metrics_luima, 'luima')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42686b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe with the error metrics\n",
    "df_luima = pd.DataFrame(error_metrics_luima)\n",
    "df_luima.sort_values('f1_score').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf07c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See whether the scores for the three docs with lowest F1 scores have improved\n",
    "# with the law specific segmenter\n",
    "\n",
    "for doc in low_scoring_docs:\n",
    "    improved_precision = df_luima[df_luima['doc_id']==doc['doc_id']]['precision'].values[0]\n",
    "    improved_recall = df_luima[df_luima['doc_id']==doc['doc_id']]['recall'].values[0]\n",
    "    improved_f1 = df_luima[df_luima['doc_id']==doc['doc_id']]['f1_score'].values[0]\n",
    "    print(f\"Comparison between standard and improved segmenter for document {doc['doc_id']}:\")\n",
    "    \n",
    "    print(f\"Old Precision: {doc['precision']}, improved precision: {improved_precision}\")\n",
    "    print(f\"Recall: {doc['recall']}, improved_recall: {improved_recall}\")\n",
    "    print(f\"F1 score: {doc['f1_score']}, improved_f1: {improved_f1}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da250e80",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Error analysis on an individual level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b721ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "low_scoring_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18120942",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Choose a low scoring doc for analysis\n",
    "doc_id = low_scoring_docs[2]['doc_id']\n",
    "doc_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b0f4da",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Error analysis with standard Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e499393f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Test for a single document\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "gen_ann_span_standard = generate_ann_span_by_doc_with_spacy([doc_id], nlp)\n",
    "# print(len(gen_ann_span_standard[doc_id]['start']))\n",
    "# gen_ann_span_standard\n",
    "\n",
    "# For the standard spacy\n",
    "closest_by_id_std = find_closest_start_point(\n",
    "    [doc_id], \n",
    "    true_ann_span_by_doc, \n",
    "    gen_ann_span_standard\n",
    ")\n",
    "\n",
    "# print(closest_by_id_std)\n",
    "# For the standard spacy\n",
    "print_str = f\"Error analysis for the document {doc_id} with standard segmenter\"\n",
    "print_pattern = \"-\"*len(print_str)\n",
    "print(print_str)\n",
    "print(print_pattern)\n",
    "_, _, _, _, _, precision, recall, f1_score = calculate_error_metrics(\n",
    "    [doc_id], \n",
    "    true_ann_span_by_doc, \n",
    "    gen_ann_span_standard, \n",
    "    closest_by_id_std\n",
    ")\n",
    "\n",
    "print(f'Precision: {precision:.2f}\\nRecall: {recall:.2f}\\nF1_score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86be684",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now Compare the true and generated splits of the chosen document\n",
    "\n",
    "i = 1\n",
    "print(f\"Comparing true and generated segments for document {doc_id} with standard segmenter\")\n",
    "for true_span, pred_span in zip(closest_by_id_std[doc_id]['true'], closest_by_id_std[doc_id]['pred']):\n",
    "    test_doc = documents_by_id[doc_id]['plainText']\n",
    "    GT = test_doc[true_span['start']: true_span['end']]\n",
    "    pred = test_doc[pred_span['start']: pred_span['end']]\n",
    "    true_start = true_span['start']\n",
    "    pred_start = pred_span['start']\n",
    "    \n",
    "    \n",
    "    dist = abs(true_start - pred_start)                      \n",
    "    if  3 < dist:\n",
    "        print(f\"true start: {true_start}, true end: {true_span['end']} pred start: {pred_start}, distance: {abs(true_start - pred_start)}\")\n",
    "        print_str = GT\n",
    "        print_pattern_out = \"=\"*80\n",
    "        print_pattern_in = \"-\"*80\n",
    "\n",
    "        print(print_pattern_out)\n",
    "        print(f\"Segment {i}\")\n",
    "        print(print_pattern_in)\n",
    "        print(\"GT\".center(50))\n",
    "        print(GT)\n",
    "        print(print_pattern_in)\n",
    "\n",
    "        print(\"PRED\".center(50))\n",
    "        print(pred, \"\\n\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b372636",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Error analysis with Extended Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494b740",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test for a single document\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# nlp.add_pipe(\"set_custom_boundaries_original\", before=\"parser\")\n",
    "nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])\n",
    "\n",
    "gen_ann_span_ext = generate_ann_span_by_doc_with_spacy([doc_id], nlp)\n",
    "len(gen_ann_span_ext[doc_id]['start'])\n",
    "\n",
    "# For the extended spacy\n",
    "closest_by_id_ext = find_closest_start_point(\n",
    "    [doc_id], \n",
    "    true_ann_span_by_doc, \n",
    "    gen_ann_span_ext\n",
    ")\n",
    "\n",
    "\n",
    "# For the extended spacy\n",
    "print(f\"Error analysis for the document {doc_id} with extended segmenter\")\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "_, _, _, _, _, precision, recall, f1_score = calculate_error_metrics(\n",
    "    [doc_id], \n",
    "    true_ann_span_by_doc, \n",
    "    gen_ann_span_ext, \n",
    "    closest_by_id_ext\n",
    ")\n",
    "\n",
    "print(f'\\nPrecision: {precision:.2f}\\nRecall: {recall:.2f}\\nF1_score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52582223",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now Compare the true and generated splits of the chosen document\n",
    "print(f\"Comparing true and generated segments for document {doc_id} with extended segmenter\")\n",
    "i = 1\n",
    "for true, pred in zip(closest_by_id_ext[doc_id]['true'], closest_by_id_ext[doc_id]['pred']):\n",
    "    test_doc = documents_by_id[doc_id]['plainText']\n",
    "    GT = test_doc[true['start']: true['end']]\n",
    "    pred = test_doc[pred['start']: pred['end']]\n",
    "    \n",
    "    print_str = GT\n",
    "    print_pattern_out = \"=\"*80\n",
    "    print_pattern_in = \"-\"*80\n",
    "    \n",
    "    print(print_pattern_out)\n",
    "    print(f\"Segment {i}\")\n",
    "    print(print_pattern_in)\n",
    "    print(\"GT\".center(50))\n",
    "    print(GT)\n",
    "    print(print_pattern_in)\n",
    "    \n",
    "    print(\"PRED\".center(50))\n",
    "    print(pred, \"\\n\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe37a73",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Error analysis with LUIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704c276d",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gen_ann_span_luima = generate_ann_span_by_doc_with_luima([doc_id])\n",
    "len(gen_ann_span_luima[doc_id]['start'])\n",
    "# gen_ann_span_ext\n",
    "\n",
    "# For the LUIMA\n",
    "closest_by_id_luima = find_closest_start_point(\n",
    "    [doc_id], \n",
    "    true_ann_span_by_doc, \n",
    "    gen_ann_span_luima\n",
    ")\n",
    "\n",
    "\n",
    "# For the LUIMA\n",
    "\n",
    "print(f\"Error analysis for the document {doc_id} with LUIMA segmenter\")\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "\n",
    "_, _, _, _, _, precision, recall, f1_score = calculate_error_metrics(\n",
    "    [doc_id], \n",
    "    true_ann_span_by_doc, \n",
    "    gen_ann_span_luima, \n",
    "    closest_by_id_luima\n",
    ")\n",
    "\n",
    "print(f'\\nPrecision: {precision:.2f}\\nRecall: {recall:.2f}\\nF1_score: {f1_score:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566a271",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Now Compare the true and generated splits of the chosen document\n",
    "print(f\"Comparing true and generated segments for document {doc_id} with LUIMA segmenter\")\n",
    "i = 1\n",
    "\n",
    "for start, end in zip(gen_ann_span_luima['61aea55c97ad59b4cfc41299']['start'], gen_ann_span_luima['61aea55c97ad59b4cfc41299']['end']):\n",
    "    test_doc = documents_by_id[doc_id]['plainText'].strip()\n",
    "    GT = test_doc[start: end]\n",
    "    \n",
    "#     print(f\"true start: {true_start}, true end: {true_span['end']} pred start: {pred_start}, distance: {abs(true_start - pred_start)}\")\n",
    "    print_str = GT\n",
    "    print_pattern_out = \"=\"*80\n",
    "    print_pattern_in = \"-\"*80\n",
    "\n",
    "    print(print_pattern_out)\n",
    "    print(f\"Segment {i}\")\n",
    "    print(print_pattern_in)\n",
    "    print(\"GT\".center(50))\n",
    "    print(GT)\n",
    "    print(print_pattern_in)\n",
    "\n",
    "#     print(\"PRED\".center(50))\n",
    "#     print(pred, \"\\n\")\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
