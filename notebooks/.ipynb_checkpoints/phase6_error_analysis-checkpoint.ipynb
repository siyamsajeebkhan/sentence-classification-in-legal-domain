{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2388e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.language import Language\n",
    "from luima_sbd import sbd_utils as luima\n",
    "import math\n",
    "import fasttext\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import random\n",
    "from joblib import dump, load\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5213e930",
   "metadata": {},
   "source": [
    "## Some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a100c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=15):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_features_in_doc(Xtr, features, row_id, top_n=15):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    xtr_row = Xtr[row_id]\n",
    "    if type(xtr_row) is not np.ndarray:\n",
    "        xtr_row = xtr_row.toarray()\n",
    "    row = np.squeeze(xtr_row)\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "\n",
    "def top_mean_features(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]\n",
    "    else:\n",
    "        D = Xtr\n",
    "    if type(D) is not np.ndarray:\n",
    "        D = D.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "def top_features_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def span_top_tfidf(spans_txt, spans_tfidf, features, index):\n",
    "    print('span text:\\n'+spans_txt[index]+'\\n')\n",
    "    print(top_features_in_doc(spans_tfidf, features, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])\n",
    "\n",
    "def custom_spacy_tokenize(txt):\n",
    "    nlp.disable_pipes('parser')\n",
    "    doc = nlp.pipe(txt, n_process=4)\n",
    "    doc = nlp(txt)\n",
    "    tokens = list(doc)\n",
    "    clean_tokens = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.pos_ == 'PUNCT' and not re.search(\"^[0-9]{2}/[0-9]{2}/([0-9]{2}|[0-9]{4})$\", token.text):\n",
    "            pass\n",
    "        \n",
    "        elif token.pos_ == 'NUM':\n",
    "            refined_token = re.sub(r'\\W', '', token.text)\n",
    "            clean_tokens.append(f'<NUM{len(refined_token)}>')\n",
    "            \n",
    "        elif token.text == \"\\'s\" and token.pos_ == 'PART':\n",
    "            pos_token = tokens[i-1].text + token.text\n",
    "            clean_tokens.pop(len(clean_tokens)-1)\n",
    "            clean_tokens.append(pos_token.lower())\n",
    "                   \n",
    "        elif \"-\" in token.text:\n",
    "            splitted_tokens = token.text.split(\"-\")\n",
    "\n",
    "            for sp_token in splitted_tokens:\n",
    "                refined_token = re.sub(r'\\W', '', sp_token.lower())\n",
    "                if refined_token != \"\":\n",
    "                    if refined_token.isnumeric():\n",
    "                        refined_token = f'<NUM{len(refined_token)}>'\n",
    "                    clean_tokens.append(refined_token)\n",
    "        elif token.text in (\"Vet. App.\", \"Fed. Cir.\"):\n",
    "            clean_tokens.append(token.lemma_.lower())\n",
    "\n",
    "        else:\n",
    "            refined_token = re.sub(r'\\W', '', token.lemma_.lower())\n",
    "            if re.search('\\d+', refined_token) and re.search('[a-zA-Z]+', refined_token):\n",
    "                continue\n",
    "            elif refined_token != \"\" and refined_token.isnumeric():\n",
    "                refined_token = f'<NUM{len(refined_token)}>'\n",
    "                clean_tokens.append(refined_token)\n",
    "\n",
    "            elif refined_token != \"\":\n",
    "                clean_tokens.append(refined_token)\n",
    "                    \n",
    "    return clean_tokens\n",
    "\n",
    "def custom_spans_add_spacy_tokens(spans):\n",
    "    for s in tqdm(spans):\n",
    "        s['tokens_spacy'] = custom_spacy_tokenize(s['txt'])\n",
    "        s['token_count'] = len(s['tokens_spacy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58328b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sentences assuming every annotation is a sentence\n",
    "def make_span_data(documents_by_id, types_by_id, annotations, doc_ids):\n",
    "    span_data = []\n",
    "    for doc_id in doc_ids:\n",
    "        for a in annotations:\n",
    "            if a['document'] == doc_id:\n",
    "                start = a['start']\n",
    "                end = a['end']\n",
    "                document_txt = documents_by_id[a['document']]['plainText']\n",
    "                atype = a['type']\n",
    "                sd = {'txt': document_txt[start:end],\n",
    "                      'document': a['document'],\n",
    "                      'type': types_by_id[atype]['name'],\n",
    "                      'start': a['start'],\n",
    "                      'start_normalized': a['start'] / len(document_txt),\n",
    "                      'end': a['end']}\n",
    "                span_data.append(sd)\n",
    "    return span_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23427978",
   "metadata": {},
   "source": [
    "## Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45eba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the JSON file containing the data and the annotations\n",
    "CURATED_ANN_PATH = \"../Data/ldsi_w21_curated_annotations_v2.json\"\n",
    "with open(CURATED_ANN_PATH, 'r') as j:\n",
    "     data = json.loads(j.read())\n",
    "        \n",
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}\n",
    "\n",
    "granted_doc_ids = set([doc['_id'] for doc in data['documents'] if doc['outcome'] == 'granted'])\n",
    "denied_doc_ids = set([doc['_id'] for doc in data['documents'] if doc['outcome'] == 'denied'])\n",
    "print(len(granted_doc_ids), len(denied_doc_ids))\n",
    "\n",
    "# Filter out the IDs of the 141 documents from a total of 540\n",
    "ids_annotated_docs = set([ann['document'] for ann in data['annotations']])\n",
    "print(len(ids_annotated_docs))\n",
    "\n",
    "granted_ids = sorted(list(granted_doc_ids.intersection(ids_annotated_docs)))\n",
    "denied_ids = sorted(list(denied_doc_ids.intersection(ids_annotated_docs)))\n",
    "print(len(granted_ids), len(denied_ids))\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "random.shuffle(granted_ids)\n",
    "random.shuffle(denied_ids)\n",
    "granted_train, granted_val, granted_test = np.split(granted_ids, [int(len(granted_ids)*0.8), int(len(granted_ids)*0.9)])\n",
    "denied_train, denied_val, denied_test = np.split(denied_ids, [57, 64])\n",
    "\n",
    "train_set, dev_set, test_set = np.concatenate((granted_train, denied_train), axis=0), \\\n",
    "                                np.concatenate((granted_val, denied_val), axis=0), \\\n",
    "                                    np.concatenate((granted_test, denied_test), axis=0), \\\n",
    "\n",
    "print(train_set.shape, dev_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2112d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, dev_ids, test_ids = train_set, dev_set, test_set\n",
    "train_ids.shape, dev_ids.shape, test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the train, dev and test ids saved in phase 1\n",
    "# train_ids, dev_ids, test_ids = np.load('../Data/train.npy'), np.load('../Data/dev.npy'), np.load('../Data/test.npy')\n",
    "# train_ids.shape, dev_ids.shape, test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d978c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train, dev and test spans\n",
    "train_spans = make_span_data(documents_by_id, types_by_id, annotations, train_ids)\n",
    "dev_spans = make_span_data(documents_by_id, types_by_id, annotations, dev_ids)\n",
    "test_spans = make_span_data(documents_by_id, types_by_id, annotations, test_ids)\n",
    "\n",
    "train_spans_txt = [s['txt'] for s in train_spans]\n",
    "dev_spans_txt = [s['txt'] for s in dev_spans]\n",
    "test_spans_txt = [s['txt'] for s in test_spans]\n",
    "\n",
    "\n",
    "train_spans_labels = np.array([s['type'] for s in train_spans])\n",
    "test_spans_labels = np.array([s['type'] for s in test_spans])\n",
    "dev_spans_labels = np.array([s['type'] for s in dev_spans])\n",
    "len(train_spans), len(dev_spans), len(test_spans), len(train_spans_txt), len(dev_spans_txt), len(test_spans_txt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812083f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the spacy tokens to the span data: contains two fields now, txt and the spacy tokens\n",
    "custom_spans_add_spacy_tokens(train_spans)\n",
    "custom_spans_add_spacy_tokens(dev_spans)\n",
    "custom_spans_add_spacy_tokens(test_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d3507",
   "metadata": {},
   "source": [
    "### Create TFIDF feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64eee5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_tfidf_feature_vectors_and_labels(spans, vectorizer, train_mean, train_std):\n",
    "#     # function takes long to execute\n",
    "#     # note: we un-sparse the matrix here to be able to manipulate it\n",
    "    \n",
    "#     df = pd.DataFrame([s['token_count'] for s in spans])\n",
    "#     df.columns = ['token_count']\n",
    "# #     token_count_mean, token_count_std = df['token_count'].mean(), df['token_count'].std()\n",
    "#     token_count_mean, token_count_std = train_mean, train_std\n",
    "\n",
    "#     tfidf = vectorizer.transform([s['txt'] for s in spans]).toarray()\n",
    "#     starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "#     token_count_normalized = np.array([(s['token_count']-token_count_mean)/token_count_std for s in spans])\n",
    "\n",
    "#     y = np.array([s['type'] for s in spans])\n",
    "#     X = np.concatenate((tfidf, np.expand_dims(starts_normalized, axis=1), np.expand_dims(token_count_normalized, axis=1)), axis=1)\n",
    "#     return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664014a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_spacy_tokenize,\n",
    "#                                          min_df=3,\n",
    "#                                          ngram_range=(1,1))\n",
    "\n",
    "# spacy_tfidf_vectorizer = spacy_tfidf_vectorizer.fit(train_spans_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77cfe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extend by adding a a single float variable representing the number of tokens in the sentence, normalized\n",
    "# # by subtracting the mean and dividing by the standard deviation across all sentence\n",
    "# # tokens counts in the training data\n",
    "\n",
    "# df = pd.DataFrame([s['token_count'] for s in train_spans])\n",
    "# df.columns = ['token_count']\n",
    "# train_token_count_mean, train_token_count_std = df['token_count'].mean(), df['token_count'].std()\n",
    "\n",
    "# print(train_token_count_mean, train_token_count_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff65c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_X_tfidf, train_y_tfidf = make_tfidf_feature_vectors_and_labels(train_spans, spacy_tfidf_vectorizer, train_token_count_mean, train_token_count_std)\n",
    "# dev_X_tfidf, dev_y_tfidf = make_tfidf_feature_vectors_and_labels(dev_spans, spacy_tfidf_vectorizer, train_token_count_mean, train_token_count_std)\n",
    "# test_X_tfidf, test_y_tfidf = make_tfidf_feature_vectors_and_labels(test_spans, spacy_tfidf_vectorizer, train_token_count_mean, train_token_count_std)\n",
    "\n",
    "# print(f'{train_X_tfidf.shape} {train_y_tfidf.shape}')\n",
    "# print(f'{dev_X_tfidf.shape} {dev_y_tfidf.shape}')\n",
    "# print(f'{test_X_tfidf.shape} {test_y_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca02254",
   "metadata": {},
   "source": [
    "### Create Word embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dded795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was calculated from the training data\n",
    "train_token_count_mean, train_token_count_std = 21.035180722891567, 15.719815094996603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853cc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_embedded_feature_vectors_and_labels(spans, vectorizer, train_mean, train_std):\n",
    "    df = pd.DataFrame([s['token_count'] for s in spans])\n",
    "    df.columns = ['token_count']\n",
    "#     token_count_mean, token_count_std = df['token_count'].mean(), df['token_count'].std()\n",
    "    token_count_mean, token_count_std = train_mean, train_std\n",
    "#     print(f\"mean token count across the sentences: {token_count_mean}, std of the token counts: {token_count_std}\")\n",
    "    final_word_vector = []\n",
    "    for s in spans:\n",
    "        if (len(s['tokens_spacy'])):\n",
    "            word_vector = np.mean(np.array([vectorizer.get_word_vector(token) for token in s['tokens_spacy']]), axis=0)\n",
    "            final_word_vector.append(word_vector)\n",
    "            \n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    token_count_normalized = np.array([(s['token_count']-token_count_mean)/token_count_std for s in spans])\n",
    "    y = np.array([s['type'] for s in spans])\n",
    "    X = np.concatenate((np.array(final_word_vector), np.expand_dims(starts_normalized, axis=1), np.expand_dims(token_count_normalized, axis=1)), axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17277ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = fasttext.load_model(\"../models/ft_word_embedding_model.bin\")\n",
    "print(len(vectorizer.get_words(on_unicode_error='ignore')))\n",
    "\n",
    "train_X_wv, train_y_wv = make_word_embedded_feature_vectors_and_labels(train_spans, vectorizer, train_token_count_mean, train_token_count_std)\n",
    "dev_X_wv, dev_y_wv = make_word_embedded_feature_vectors_and_labels(dev_spans, vectorizer, train_token_count_mean, train_token_count_std)\n",
    "test_X_wv, test_y_wv = make_word_embedded_feature_vectors_and_labels(test_spans, vectorizer, train_token_count_mean, train_token_count_std)\n",
    "\n",
    "print(f'{train_X_wv.shape} {train_y_wv.shape}')\n",
    "print(f'{dev_X_wv.shape} {dev_y_wv.shape}')\n",
    "print(f'{test_X_wv.shape} {test_y_wv.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d40edc",
   "metadata": {},
   "source": [
    "### Loading saved featurized vectors of TF-IDF and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fe74ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the train, dev and test data of Word embedding featurizer\n",
    "# train_X_wv, train_y_wv = np.load('../Data/train_X_wv.npy'), np.load('../Data/train_y_wv.npy')\n",
    "# dev_X_wv, dev_y_wv = np.load('../Data/dev_X_wv.npy'), np.load('../Data/dev_y_wv.npy')\n",
    "# test_X_wv, test_y_wv = np.load('../Data/test_X_wv.npy'), np.load('../Data/test_y_wv.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{train_X_wv.shape} {train_y_wv.shape}')\n",
    "# print(f'{dev_X_wv.shape} {dev_y_wv.shape}')\n",
    "# print(f'{test_X_wv.shape} {test_y_wv.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading the train, dev and test data of TF-IDF featurizer\n",
    "# train_X_tfidf, train_y_tfidf = np.load('../Data/train_X_tfidf.npy'), np.load('../Data/train_y_tfidf.npy')\n",
    "# dev_X_tfidf, dev_y_tfidf = np.load('../Data/dev_X_tfidf.npy'), np.load('../Data/dev_y_tfidf.npy')\n",
    "# test_X_tfidf, test_y_tfidf = np.load('../Data/test_X_tfidf.npy'), np.load('../Data/test_y_tfidf.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06edebfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'{train_X_tfidf.shape} {train_y_tfidf.shape}')\n",
    "# print(f'{dev_X_tfidf.shape} {dev_y_tfidf.shape}')\n",
    "# print(f'{test_X_tfidf.shape} {test_y_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fd0721",
   "metadata": {},
   "source": [
    "## Testing the Best model on dev and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884c202",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the best model on word embedding features\n",
    "# vectorizer = fasttext.load_model(\"../models/ft_word_embedding_model.bin\")\n",
    "# feature_vector = make_word_embedded_feature_vectors_and_labels(spans, vectorizer)\n",
    "clf_wv = load('../models/word_embedding_best_model_svc_rbf_notebook.joblib') \n",
    "\n",
    "train_preds = clf_wv.predict(train_X_wv)\n",
    "dev_preds = clf_wv.predict(dev_X_wv)\n",
    "test_preds = clf_wv.predict(test_X_wv)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_wv, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_wv, dev_preds, zero_division=1))\n",
    "print('TEST:\\n'+classification_report(test_y_wv, test_preds, zero_division=1))\n",
    "\n",
    "plot_confusion_matrix(train_y_wv, train_preds, classes=list(clf_wv.classes_),\n",
    "                      title='Confusion matrix for train data')\n",
    "plot_confusion_matrix(dev_y_wv, dev_preds, classes=list(clf_wv.classes_),\n",
    "                      title='Confusion matrix of Radial Kernel SVM\\n on dev data with Word Embedding featurization')\n",
    "\n",
    "plot_confusion_matrix(test_y_wv, test_preds, classes=list(clf_wv.classes_),\n",
    "                      title='Confusion matrix for test data')\n",
    "\n",
    "plt.savefig('confusion_matrix_best_model_word_embedding_dev_set.jpg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8929109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Loading the best model on TF-IDF features\n",
    "# # vectorizer = fasttext.load_model(\"../models/ft_word_embedding_model.bin\")\n",
    "# # feature_vector = make_word_embedded_feature_vectors_and_labels(spans, vectorizer)\n",
    "# clf_tfidf = load('../models/tfidf_best_model_svc_rbf.joblib') \n",
    "\n",
    "# train_preds = clf_tfidf.predict(train_X_tfidf)\n",
    "# dev_preds = clf_tfidf.predict(dev_X_tfidf)\n",
    "# test_preds = clf_tfidf.predict(test_X_tfidf)\n",
    "\n",
    "# print('TRAIN:\\n'+classification_report(train_y_tfidf, train_preds, zero_division=1))\n",
    "# print('DEV:\\n'+classification_report(dev_y_tfidf, dev_preds, zero_division=1))\n",
    "# print('TEST:\\n'+classification_report(test_y_tfidf, test_preds, zero_division=1))\n",
    "\n",
    "# # plot_confusion_matrix(train_y_tfidf, train_preds, classes=list(clf.classes_),\n",
    "# #                       title='Confusion matrix for TFIDF train data')\n",
    "\n",
    "# plot_confusion_matrix(dev_y_tfidf, dev_preds, classes=list(clf.classes_),\n",
    "#                       title='Confusion matrix of Radial Kernel SVM\\n on dev data with TFIDF featurization')\n",
    "\n",
    "# # plot_confusion_matrix(test_y_tfidf, test_preds, classes=list(clf.classes_),\n",
    "# #                       title='Confusion matrix for TFIDF test data')\n",
    "\n",
    "# plt.savefig('confusion_matrix_best_model_tfidf_dev_set.jpg')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3400e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbba0dc5",
   "metadata": {},
   "source": [
    "## Error analysis for individual classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53eb7192",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_embedded_feature_vectors_and_labels(spans, model):\n",
    "    df = pd.DataFrame([s['token_count'] for s in spans])\n",
    "    df.columns = ['token_count']\n",
    "    token_count_mean, token_count_std = df['token_count'].mean(), df['token_count'].std()\n",
    "    final_word_vector = []\n",
    "    for s in spans:\n",
    "        if (len(s['tokens_spacy'])):\n",
    "            word_vector = np.mean(np.array([model.get_word_vector(token) for token in s['tokens_spacy']]), axis=0)\n",
    "            final_word_vector.append(word_vector)\n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    token_count_normalized = np.array([(s['token_count']-token_count_mean)/token_count_std for s in spans])\n",
    "    y = np.array([s['type'] for s in spans])\n",
    "    X = np.concatenate((np.array(final_word_vector), np.expand_dims(starts_normalized, axis=1), np.expand_dims(token_count_normalized, axis=1)), axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6499a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_errors(clf, eval_spans, vectorizer, \n",
    "                      select_true_label=None, \n",
    "                      select_pred_label=None):\n",
    "    eval_X, eval_y = make_word_embedded_feature_vectors_and_labels(eval_spans, vectorizer)\n",
    "    eval_spans_txt = [s['txt'] for s in eval_spans]\n",
    "    eval_spans_labels = [s['type'] for s in eval_spans]\n",
    "    pred_y = clf.predict(eval_X)\n",
    "    for i in range(len(eval_spans)):\n",
    "        true_label = eval_spans_labels[i]\n",
    "        pred_label = pred_y[i]\n",
    "        if true_label != pred_label:\n",
    "            if select_true_label and true_label != select_true_label: continue\n",
    "            if select_pred_label and pred_label != select_pred_label: continue\n",
    "            doc_name = documents_by_id[eval_spans[i]['document']]['name']\n",
    "            print('sentence # '+str(i)+' / case '+doc_name+' / @'+str(eval_spans[i]['start']))\n",
    "            print('pred: '+pred_label+' / true: '+true_label)\n",
    "            print(eval_spans[i]['txt'])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a7089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy_tfidf_vectorizer = load('../models/tfidf_featurizer.joblib')\n",
    "wv_vectorizer = fasttext.load_model(\"../models/ft_word_embedding_model_notebook.bin\")\n",
    "clf_wv = load('../models/word_embedding_best_model_svc_rbf_notebook.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef4ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_misclassified_labels = [\n",
    "    'RemandInstructions', \n",
    "    'PolicyBasedReasoning', \n",
    "    'LegislationAndPolicy', \n",
    "    'EvidenceBasedReasoning'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d068539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_sampler_for_testing(spans, label, no_of_samples=None, shuffle=True):\n",
    "    sampled_spans = []\n",
    "    sample_count = 0\n",
    "    for s in spans:\n",
    "        if s['type'] == label:\n",
    "            sampled_spans.append(s)\n",
    "            sample_count+=1\n",
    "#             if sample_count == no_of_samples:\n",
    "#                 print(f\"Found {sample_count} samples and returning\")\n",
    "#                 return sampled_spans\n",
    "\n",
    "    if no_of_samples is None:\n",
    "        return sampled_spans\n",
    "    if shuffle and len(sampled_spans) >= no_of_samples:\n",
    "        return random.sample(sampled_spans, no_of_samples)\n",
    "    if not shuffle and len(sampled_spans) >= no_of_samples:\n",
    "        return sampled_spans[0:no_of_samples]\n",
    "    if len(sampled_spans) < no_of_samples:        \n",
    "        return sampled_spans\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab1a34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences = sentence_sampler_for_testing(dev_spans, 'PolicyBasedReasoning', shuffle=True)\n",
    "\n",
    "for sent in sentences:\n",
    "    print(sent['txt'], \"--->\", sent['type'], '====>', sent['document'], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af943e4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test for RemandInstructions\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "#                   random.sample(train_spans, 500),\n",
    "                  sentence_sampler_for_testing(dev_spans, 'LegalRule', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='EvidenceBasedReasoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70667bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for RemandInstructions\n",
    "prediction_errors(clf_wv,\n",
    "#                   random.sample(train_spans, 500),\n",
    "                  sentence_sampler_for_testing(dev_spans, 'LegalRule', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='LegislationAndPolicy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38d542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for RemandInstructions\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "#                   random.sample(train_spans, 500),\n",
    "                  sentence_sampler_for_testing(dev_spans, 'EvidenceBasedReasoning', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='EvidenceBasedOrIntermediateFinding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18015b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for RemandInstructions\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "#                   random.sample(train_spans, 500),\n",
    "                  sentence_sampler_for_testing(dev_spans, 'Evidence', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='Citation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc46130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for RemandInstructions\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "#                   random.sample(train_spans, 500),\n",
    "                  sentence_sampler_for_testing(dev_spans, 'CaseHeader', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='Header')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45835667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test for RemandInstructions\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "#                   random.sample(train_spans, 500),\n",
    "                  sentence_sampler_for_testing(dev_spans, 'EvidenceBasedReasoning', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='LegalRule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for RemandInstructions\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "#                   random.sample(train_spans, 500),\n",
    "                  sentence_sampler_for_testing(dev_spans, 'ConclusionOfLaw', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='LegalRule')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5d2d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for RemandInstructions\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "#                   random.sample(train_spans, 500),\n",
    "                  sentence_sampler_for_testing(dev_spans, 'EvidenceBasedReasoning', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='Procedure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d58cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for PolicyBasedReasoning\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "                  random.sample(train_spans, 500),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='PolicyBasedReasoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382cf13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for LegislationAndPolicy\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "                  random.sample(train_spans, 500),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='LegislationAndPolicy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c070aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test for LegislationAndPolicy\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "                  random.sample(train_spans, 500),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='EvidenceBasedReasoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7e3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for EvidenceBasedReasoning\n",
    "\n",
    "prediction_errors(clf_wv,\n",
    "                  sentence_sampler_for_testing(train_spans, 'EvidenceBasedOrIntermediateFinding', shuffle=True),\n",
    "                  wv_vectorizer,\n",
    "                  select_pred_label='EvidenceBasedReasoning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf834a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b7a47b5d",
   "metadata": {},
   "source": [
    "Most mispredicted classes are:\n",
    "\n",
    "1. EvidenceBasedReasoning --> \n",
    "    LegalRule(1), \n",
    "    Header(1), \n",
    "    EvidenceBasedOrIntermediateFinding(19), \n",
    "    Evidence(7)\n",
    "\n",
    "2. LegislationAndPolicy --> \n",
    "    LegalRule(2), \n",
    "    Evidence(1)\n",
    "\n",
    "3. EvidenceBasedOrIntermediateFinding --> \n",
    "    Procedure(2), \n",
    "    PolicyBasedReasoning(1), \n",
    "    EvidenceBasedReasoning(26), \n",
    "    Evidence(4), \n",
    "    LegalRule(6), \n",
    "    LegislationAndPolicy(1), \n",
    "    ConclusionOfLaw(3)\n",
    "Others:\n",
    "1. Citation -->\n",
    "    Evidence\n",
    "    LegalRule\n",
    "    LegislationAndPolicy\n",
    "    Procedure\n",
    "\n",
    "2. Header -->\n",
    "    CaseHeader\n",
    "    \n",
    "3. LegalRule -->\n",
    "    LegislationAndPolicy(10)\n",
    "    PolicyBasedReasoning(1)\n",
    "    Procedure(1)\n",
    "    EvidenceBasedReasoning(16)\n",
    "    EvidenceBasedOrIntermediateFinding(9)\n",
    "    Evidence(13)\n",
    "    ConclusionOfLaw(3)\n",
    "    Citation(2)\n",
    "    \n",
    "4. Procedure -->\n",
    "    EvidenceBasedReasoning(1)\n",
    "    EvidenceBasedOrIntermediateFinding(1)\n",
    "    Evidence(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25849c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
