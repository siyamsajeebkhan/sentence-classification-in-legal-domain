{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2388e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../code')\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import spacy\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from sklearn import model_selection\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "from spacy.symbols import ORTH\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "from spacy.language import Language\n",
    "from luima_sbd import sbd_utils as luima\n",
    "import math\n",
    "import fasttext\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import tree\n",
    "import random\n",
    "from joblib import dump, load\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5213e930",
   "metadata": {},
   "source": [
    "## Some necessary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336141d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_errors(clf, eval_spans, vectorizer, \n",
    "                      select_true_label=None, \n",
    "                      select_pred_label=None):\n",
    "    eval_X, eval_y = make_feature_vectors_and_labels(eval_spans, vectorizer)\n",
    "    eval_spans_txt = [s['txt'] for s in eval_spans]\n",
    "    eval_spans_labels = [s['type'] for s in eval_spans]\n",
    "    pred_y = clf.predict(eval_X)\n",
    "    for i in range(len(eval_spans)):\n",
    "        true_label = eval_spans_labels[i]\n",
    "        pred_label = pred_y[i]\n",
    "        if true_label != pred_label:\n",
    "            if select_true_label and true_label != select_true_label: continue\n",
    "            if select_pred_label and pred_label != select_pred_label: continue\n",
    "            doc_name = documents_by_id[eval_spans[i]['document']]['name']\n",
    "            print('sentence # '+str(i)+' / case '+doc_name+' / @'+str(eval_spans[i]['start']))\n",
    "            print('pred: '+pred_label+' / true: '+true_label)\n",
    "            print(eval_spans[i]['txt'])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c5f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a100c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_features(row, features, top_n=15):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df\n",
    "\n",
    "\n",
    "def top_features_in_doc(Xtr, features, row_id, top_n=15):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    xtr_row = Xtr[row_id]\n",
    "    if type(xtr_row) is not np.ndarray:\n",
    "        xtr_row = xtr_row.toarray()\n",
    "    row = np.squeeze(xtr_row)\n",
    "    return top_tfidf_features(row, features, top_n)\n",
    "\n",
    "\n",
    "def top_mean_features(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids]\n",
    "    else:\n",
    "        D = Xtr\n",
    "    if type(D) is not np.ndarray:\n",
    "        D = D.toarray()\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_features(tfidf_means, features, top_n)\n",
    "\n",
    "\n",
    "def top_features_by_class(Xtr, y, features, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return a list of dfs, where each df holds top_n features and their mean tfidf value\n",
    "        calculated across documents with the same class label. '''\n",
    "    dfs = {}\n",
    "    labels = np.unique(y)\n",
    "    for label in labels:\n",
    "        ids = np.where(y==label)\n",
    "        feats_df = top_mean_features(Xtr, features, ids, min_tfidf=min_tfidf, top_n=top_n)\n",
    "        feats_df.label = label\n",
    "        dfs[label] = feats_df\n",
    "    return dfs\n",
    "\n",
    "\n",
    "def span_top_tfidf(spans_txt, spans_tfidf, features, index):\n",
    "    print('span text:\\n'+spans_txt[index]+'\\n')\n",
    "    print(top_features_in_doc(spans_tfidf, features, index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2e792",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.tokenizer.add_special_case('Vet. App.', [{ORTH: 'Vet. App.'}])\n",
    "nlp.tokenizer.add_special_case('Fed. Cir.', [{ORTH: 'Fed. Cir.'}])\n",
    "\n",
    "def custom_spacy_tokenize(txt):\n",
    "    nlp.disable_pipes('parser')\n",
    "    doc = nlp.pipe(txt, n_process=4)\n",
    "    doc = nlp(txt)\n",
    "    tokens = list(doc)\n",
    "    clean_tokens = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token.pos_ == 'PUNCT' and not re.search(\"^[0-9]{2}/[0-9]{2}/([0-9]{2}|[0-9]{4})$\", token.text):\n",
    "            pass\n",
    "        \n",
    "        elif token.pos_ == 'NUM':\n",
    "            refined_token = re.sub(r'\\W', '', token.text)\n",
    "            clean_tokens.append(f'<NUM{len(refined_token)}>')\n",
    "            \n",
    "        elif token.text == \"\\'s\" and token.pos_ == 'PART':\n",
    "            pos_token = tokens[i-1].text + token.text\n",
    "            clean_tokens.pop(len(clean_tokens)-1)\n",
    "            clean_tokens.append(pos_token.lower())\n",
    "                   \n",
    "        elif \"-\" in token.text:\n",
    "            splitted_tokens = token.text.split(\"-\")\n",
    "\n",
    "            for sp_token in splitted_tokens:\n",
    "                refined_token = re.sub(r'\\W', '', sp_token.lower())\n",
    "                if refined_token != \"\":\n",
    "                    if refined_token.isnumeric():\n",
    "                        refined_token = f'<NUM{len(refined_token)}>'\n",
    "                    clean_tokens.append(refined_token)\n",
    "        elif token.text in (\"Vet. App.\", \"Fed. Cir.\"):\n",
    "            clean_tokens.append(token.lemma_.lower())\n",
    "\n",
    "        else:\n",
    "            refined_token = re.sub(r'\\W', '', token.lemma_.lower())\n",
    "            if re.search('\\d+', refined_token) and re.search('[a-zA-Z]+', refined_token):\n",
    "                continue\n",
    "            elif refined_token != \"\" and refined_token.isnumeric():\n",
    "                refined_token = f'<NUM{len(refined_token)}>'\n",
    "                clean_tokens.append(refined_token)\n",
    "\n",
    "            elif refined_token != \"\":\n",
    "                clean_tokens.append(refined_token)\n",
    "                    \n",
    "    return clean_tokens\n",
    "\n",
    "def custom_spans_add_spacy_tokens(spans):\n",
    "    for s in tqdm(spans):\n",
    "        s['tokens_spacy'] = custom_spacy_tokenize(s['txt'])\n",
    "        s['token_count'] = len(s['tokens_spacy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58328b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sentences assuming every annotation is a sentence\n",
    "def make_span_data(documents_by_id, types_by_id, annotations, doc_ids):\n",
    "    span_data = []\n",
    "    for doc_id in doc_ids:\n",
    "        for a in annotations:\n",
    "            if a['document'] == doc_id:\n",
    "                start = a['start']\n",
    "                end = a['end']\n",
    "                document_txt = documents_by_id[a['document']]['plainText']\n",
    "                atype = a['type']\n",
    "                sd = {'txt': document_txt[start:end],\n",
    "                      'document': a['document'],\n",
    "                      'type': types_by_id[atype]['name'],\n",
    "                      'start': a['start'],\n",
    "                      'start_normalized': a['start'] / len(document_txt),\n",
    "                      'end': a['end']}\n",
    "                span_data.append(sd)\n",
    "    return span_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23427978",
   "metadata": {},
   "source": [
    "## Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45eba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the JSON file containing the data and the annotations\n",
    "CURATED_ANN_PATH = \"../Data/ldsi_w21_curated_annotations_v2.json\"\n",
    "with open(CURATED_ANN_PATH, 'r') as j:\n",
    "     data = json.loads(j.read())\n",
    "        \n",
    "annotations = data['annotations']\n",
    "documents_by_id = {d['_id']: d for d in data['documents']}\n",
    "types_by_id = {t['_id']: t for t in data['types']}\n",
    "type_ids_by_name = {t['name']: t['_id'] for t in data['types']}\n",
    "type_names_by_id = {t['_id']: t['name'] for t in data['types']}\n",
    "doc_id_by_name = {d['name']: d['_id'] for d in data['documents']}\n",
    "doc_name_by_id = {d['_id']: d['name'] for d in data['documents']}\n",
    "\n",
    "granted_doc_ids = set([doc['_id'] for doc in data['documents'] if doc['outcome'] == 'granted'])\n",
    "denied_doc_ids = set([doc['_id'] for doc in data['documents'] if doc['outcome'] == 'denied'])\n",
    "print(len(granted_doc_ids), len(denied_doc_ids))\n",
    "\n",
    "# Filter out the IDs of the 141 documents from a total of 540\n",
    "ids_annotated_docs = set([ann['document'] for ann in data['annotations']])\n",
    "print(len(ids_annotated_docs))\n",
    "\n",
    "granted_ids = sorted(list(granted_doc_ids.intersection(ids_annotated_docs)))\n",
    "denied_ids = sorted(list(denied_doc_ids.intersection(ids_annotated_docs)))\n",
    "print(len(granted_ids), len(denied_ids))\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "random.shuffle(granted_ids)\n",
    "random.shuffle(denied_ids)\n",
    "granted_train, granted_val, granted_test = np.split(granted_ids, [int(len(granted_ids)*0.8), int(len(granted_ids)*0.9)])\n",
    "denied_train, denied_val, denied_test = np.split(denied_ids, [57, 64])\n",
    "\n",
    "train_set, dev_set, test_set = np.concatenate((granted_train, denied_train), axis=0), \\\n",
    "                                np.concatenate((granted_val, denied_val), axis=0), \\\n",
    "                                    np.concatenate((granted_test, denied_test), axis=0), \\\n",
    "\n",
    "print(train_set.shape, dev_set.shape, test_set.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04095edd",
   "metadata": {},
   "source": [
    "##### Loading the train, dev and test ids saved in phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cd052a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, dev_ids, test_ids = train_set, dev_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febd67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ids, dev_ids, test_ids = np.load('../Data/train.npy'), np.load('../Data/dev.npy'), np.load('../Data/test.npy')\n",
    "# train_ids.shape, dev_ids.shape, test_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d978c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train, dev and test spans\n",
    "train_spans = make_span_data(documents_by_id, types_by_id, annotations, train_ids)\n",
    "dev_spans = make_span_data(documents_by_id, types_by_id, annotations, dev_ids)\n",
    "test_spans = make_span_data(documents_by_id, types_by_id, annotations, test_ids)\n",
    "\n",
    "train_spans_txt = [s['txt'] for s in train_spans]\n",
    "dev_spans_txt = [s['txt'] for s in dev_spans]\n",
    "test_spans_txt = [s['txt'] for s in test_spans]\n",
    "\n",
    "\n",
    "train_spans_labels = np.array([s['type'] for s in train_spans])\n",
    "test_spans_labels = np.array([s['type'] for s in test_spans])\n",
    "dev_spans_labels = np.array([s['type'] for s in dev_spans])\n",
    "len(train_spans), len(dev_spans), len(test_spans), len(train_spans_txt), len(dev_spans_txt), len(test_spans_txt) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940acb78",
   "metadata": {},
   "source": [
    "##### Adding the spacy tokens to the span data: contains two fields now, txt and the spacy tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812083f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_spans_add_spacy_tokens(train_spans)\n",
    "custom_spans_add_spacy_tokens(dev_spans)\n",
    "custom_spans_add_spacy_tokens(test_spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d40edc",
   "metadata": {},
   "source": [
    "### TF-IDF featurization with Spacy's custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c8817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# suboptimal: tokenizer gets called twice\n",
    "spacy_tfidf_vectorizer = TfidfVectorizer(tokenizer=custom_spacy_tokenize,\n",
    "                                         min_df=3,\n",
    "                                         ngram_range=(1,1))\n",
    "\n",
    "spacy_tfidf_vectorizer = spacy_tfidf_vectorizer.fit(train_spans_txt)\n",
    "\n",
    "# Dumping the tdidf vectorizer for later usage\n",
    "# dump(spacy_tfidf_vectorizer, '../models/tfidf_featurizer.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7249581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the saved vectorizer\n",
    "# spacy_tfidf_vectorizer = load('../models/tfidf_featurizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d775a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features_spacy = spacy_tfidf_vectorizer.get_feature_names()\n",
    "print(len(tfidf_features_spacy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecd6c4",
   "metadata": {},
   "source": [
    "##### Extending the TF-IDF feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94401dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend by adding a a single float variable representing the number of tokens in the sentence, normalized\n",
    "# by subtracting the mean and dividing by the standard deviation across all sentence\n",
    "# tokens counts in the training data\n",
    "\n",
    "df = pd.DataFrame([s['token_count'] for s in train_spans])\n",
    "df.columns = ['token_count']\n",
    "train_token_count_mean, train_token_count_std = df['token_count'].mean(), df['token_count'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27689620",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_token_count_mean, train_token_count_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363c97d1",
   "metadata": {},
   "source": [
    "##### Make extended feature vectors for train, dev and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tfidf_feature_vectors_and_labels(spans, vectorizer, train_mean, train_std):\n",
    "    # function takes long to execute\n",
    "    # note: we un-sparse the matrix here to be able to manipulate it\n",
    "    \n",
    "    df = pd.DataFrame([s['token_count'] for s in spans])\n",
    "    df.columns = ['token_count']\n",
    "#     token_count_mean, token_count_std = df['token_count'].mean(), df['token_count'].std()\n",
    "    token_count_mean, token_count_std = train_mean, train_std\n",
    "\n",
    "    tfidf = vectorizer.transform([s['txt'] for s in spans]).toarray()\n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    token_count_normalized = np.array([(s['token_count']-token_count_mean)/token_count_std for s in spans])\n",
    "\n",
    "    y = np.array([s['type'] for s in spans])\n",
    "    X = np.concatenate((tfidf, np.expand_dims(starts_normalized, axis=1), np.expand_dims(token_count_normalized, axis=1)), axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b997eec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_tfidf, train_y_tfidf = make_tfidf_feature_vectors_and_labels(train_spans, spacy_tfidf_vectorizer, train_token_count_mean, train_token_count_std)\n",
    "dev_X_tfidf, dev_y_tfidf = make_tfidf_feature_vectors_and_labels(dev_spans, spacy_tfidf_vectorizer, train_token_count_mean, train_token_count_std)\n",
    "test_X_tfidf, test_y_tfidf = make_tfidf_feature_vectors_and_labels(test_spans, spacy_tfidf_vectorizer, train_token_count_mean, train_token_count_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d002bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the train, dev and test data for later usage and faster laoding\n",
    "\n",
    "# np.save('../Data/train_X_tfidf.npy', train_X_tfidf)\n",
    "# np.save('../Data/train_y_tfidf.npy', train_y_tfidf)\n",
    "\n",
    "# np.save('../Data/dev_X_tfidf.npy', dev_X_tfidf)\n",
    "# np.save('../Data/dev_y_tfidf.npy', dev_y_tfidf)\n",
    "\n",
    "# np.save('../Data/test_X_tfidf.npy', test_X_tfidf)\n",
    "# np.save('../Data/test_y_tfidf.npy', test_y_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e0be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{train_X_tfidf.shape} {train_y_tfidf.shape}')\n",
    "print(f'{dev_X_tfidf.shape} {dev_y_tfidf.shape}')\n",
    "print(f'{test_X_tfidf.shape} {test_y_tfidf.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8993d",
   "metadata": {},
   "source": [
    "### Using fasttext word embeddings and extending it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b33a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_embedded_feature_vectors_and_labels(spans, vectorizer, train_mean, train_std):\n",
    "    df = pd.DataFrame([s['token_count'] for s in spans])\n",
    "    df.columns = ['token_count']\n",
    "#     token_count_mean, token_count_std = df['token_count'].mean(), df['token_count'].std()\n",
    "    token_count_mean, token_count_std = train_mean, train_std\n",
    "#     print(f\"mean token count across the sentences: {token_count_mean}, std of the token counts: {token_count_std}\")\n",
    "    final_word_vector = []\n",
    "    for s in spans:\n",
    "        if (len(s['tokens_spacy'])):\n",
    "            word_vector = np.mean(np.array([vectorizer.get_word_vector(token) for token in s['tokens_spacy']]), axis=0)\n",
    "            final_word_vector.append(word_vector)\n",
    "            \n",
    "    starts_normalized = np.array([s['start_normalized'] for s in spans])\n",
    "    token_count_normalized = np.array([(s['token_count']-token_count_mean)/token_count_std for s in spans])\n",
    "    y = np.array([s['type'] for s in spans])\n",
    "    X = np.concatenate((np.array(final_word_vector), np.expand_dims(starts_normalized, axis=1), np.expand_dims(token_count_normalized, axis=1)), axis=1)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb0b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = fasttext.load_model(\"../Data/ft_word_embedding_model.bin\")\n",
    "print(len(vectorizer.get_words(on_unicode_error='ignore')))\n",
    "\n",
    "train_X_wv, train_y_wv = make_word_embedded_feature_vectors_and_labels(train_spans, vectorizer, train_token_count_mean, train_token_count_std)\n",
    "dev_X_wv, dev_y_wv = make_word_embedded_feature_vectors_and_labels(dev_spans, vectorizer, train_token_count_mean, train_token_count_std)\n",
    "test_X_wv, test_y_wv = make_word_embedded_feature_vectors_and_labels(test_spans, vectorizer, train_token_count_mean, train_token_count_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc94e674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the train, dev and test data for later usage and faster laoding\n",
    "# np.save('../Data/train_X_wv.npy', train_X_wv)\n",
    "# np.save('../Data/train_y_wv.npy', train_y_wv)\n",
    "\n",
    "# np.save('../Data/dev_X_wv.npy', dev_X_wv)\n",
    "# np.save('../Data/dev_y_wv.npy', dev_y_wv)\n",
    "\n",
    "# np.save('../Data/test_X_wv.npy', test_X_wv)\n",
    "# np.save('../Data/test_y_wv.npy', test_y_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50399564",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{train_X_wv.shape} {train_y_wv.shape}')\n",
    "print(f'{dev_X_wv.shape} {dev_y_wv.shape}')\n",
    "print(f'{test_X_wv.shape} {test_y_wv.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96833347",
   "metadata": {},
   "source": [
    "##### A sanity check with a classifier to see if everything is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c66d25e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With TF-IDF vectors\n",
    "\n",
    "#clf = GaussianNB()\n",
    "clf = tree.DecisionTreeClassifier(max_depth=12)\n",
    "clf = clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_tfidf, clf.predict(train_X_tfidf), zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_tfidf, clf.predict(dev_X_tfidf), zero_division=1))\n",
    "\n",
    "plot_confusion_matrix(dev_y_tfidf, clf.predict(dev_X_tfidf), classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c3541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With Word embedded vectors\n",
    "clf = tree.DecisionTreeClassifier(max_depth=12)\n",
    "clf = clf.fit(train_X_wv, train_y_wv)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_wv, clf.predict(train_X_wv), zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_wv, clf.predict(dev_X_wv), zero_division=1))\n",
    "\n",
    "plot_confusion_matrix(dev_y_wv, clf.predict(dev_X_wv), classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94ce15",
   "metadata": {},
   "source": [
    "## Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100461de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, x_train, y_train, X_test, y_test):\n",
    "    print(\"\\nUsing {:s}\\n\".format(model['name']))\n",
    "    clf = model['model']\n",
    "    model_ident = model['name']\n",
    "    \n",
    "    clf.fit(x_train, y_train)\n",
    "    predicted = clf.predict(X_test)\n",
    "#     bac_score = balanced_accuracy_score(y_test, predicted)\n",
    "    f1_score = metrics.f1_score(y_test, predicted, average='micro')\n",
    "\n",
    "    print('F1 score: {}'.format(f1_score))\n",
    "    \n",
    "    \n",
    "    print('TRAIN:\\n'+classification_report(y_train, clf.predict(x_train), zero_division=1))\n",
    "    print('DEV:\\n'+classification_report(y_test, clf.predict(X_test), zero_division=1))\n",
    "    plot_confusion_matrix(y_test, clf.predict(X_test), classes=list(clf.classes_),\n",
    "                          title=f'Confusion matrix for {model_ident}')\n",
    "    plt.show()\n",
    "    print(\"--------------------------------------------------------------------------------------\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(metrics.classification_report(y_test, predicted))\n",
    "#     print(\"Confusion Matrix : \\n\" + str(confusion_matrix(y_test, predicted)))\n",
    "\n",
    "#     plot_precision_recall_curve(clf, X_test, y_test, ax=ax)\n",
    "    return f1_score, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f45f729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(X_train, y_train, X_test, y_test):\n",
    "    models = [\n",
    "        {\n",
    "            'model': LinearSVC(random_state=42, verbose=3, max_iter=5000),\n",
    "            'name': 'Linear SVC'\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            'model': LogisticRegression(random_state=42, max_iter=500),\n",
    "            'name': 'Logistic Regression'\n",
    "        },\n",
    "        \n",
    "#         {\n",
    "#             'model': SVC(kernel='rbf', random_state=42),\n",
    "#             'name': \"Radial Kernet SVM\"\n",
    "#         },\n",
    "        \n",
    "#         {\n",
    "#             'model': SVC(kernel='sigmoid', random_state=42),\n",
    "#             'name': \"Sigmoid Kernel SVM\"\n",
    "#         },\n",
    "        \n",
    "#         {\n",
    "#             'model': SVC(kernel='poly', random_state=42),\n",
    "#             'name': \"Polynomial Kernel SVM\"\n",
    "#         },\n",
    "        \n",
    "#         {\n",
    "#             'model': DecisionTreeClassifier(max_depth=13, random_state=42),\n",
    "#             #  'model': DecisionTreeClassifier(criterion=\"entropy\", splitter=\"random\", max_features = 'log2', random_state=42),\n",
    "#             'name': 'Decision Tree'\n",
    "#         },\n",
    "        \n",
    "#         {\n",
    "#             'model': RandomForestClassifier(\n",
    "#                 n_estimators=20,\n",
    "#                 max_depth=10,\n",
    "#                 random_state=42\n",
    "# #                 , n_jobs=-1\n",
    "#                 # , max_features='log2'\n",
    "#                 # , class_weight={0: 1, 1: 2}\n",
    "#                 # ,max_features=20\n",
    "#                 # ,criterion='entropy'\n",
    "#             ),\n",
    "#             'name': 'Random Forest Classifier'\n",
    "#         },\n",
    "        \n",
    "    ]\n",
    "    best_score, best_model = 0, None\n",
    "\n",
    "    for m in models:\n",
    "        f1_score, model = train(m, X_train, y_train, X_test, y_test)\n",
    "\n",
    "        if f1_score > best_score:\n",
    "            print('Current best model {:s}'.format(m['name']))\n",
    "            best_score, best_model = f1_score, model\n",
    "\n",
    "        # if m['name'] == 'Random Forest Classifier':\n",
    "        #     print(rf_cv.best_params_)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "    return best_score, best_model['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0024a7c",
   "metadata": {},
   "source": [
    "##### Classification with TF-IDF on linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f51489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_score, best_model =  run_train(train_X_tfidf, train_y_tfidf, dev_X_tfidf, dev_y_tfidf)\n",
    "print(f\"Best achieved score: {best_score} with model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154f27c2",
   "metadata": {},
   "source": [
    "##### Classification with Word embedding on linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeba6a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%time best_score, best_model =  run_train(train_X_wv, train_y_wv, dev_X_wv, dev_y_wv)\n",
    "print(f\"Best achieved score: {best_score} with model: {best_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35046de6",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning for Non-Linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a1ba4b",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec84c4a4",
   "metadata": {},
   "source": [
    "##### Decision Tree with TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019f3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "np.random.seed(42)\n",
    "# parameters = {'max_depth':[np.random.randint(20) for i in range(6)]}\n",
    "parameters = {'max_depth':[10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "clf = GridSearchCV(tree, parameters, verbose=3)\n",
    "clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "clf.best_estimator_\n",
    "# clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab003bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params_values = [None, 10, 16, 18]\n",
    "best_f1_scores = []\n",
    "for best_param in best_params_values:\n",
    "    clf_best = DecisionTreeClassifier(max_depth=best_param, random_state=42)\n",
    "    clf_best = clf_best.fit(train_X_tfidf, train_y_tfidf)\n",
    "    predicted = clf_best.predict(dev_X_tfidf)\n",
    "    f1_score = metrics.f1_score(dev_y_tfidf, predicted, average='micro')\n",
    "    dic = {\n",
    "        'max_depth': best_param,\n",
    "        'f1_score': round(f1_score, 2)\n",
    "    }\n",
    "    best_f1_scores.append(dic)\n",
    "    \n",
    "    print(f\"Classification report for max_depth = {best_param}\\n\")\n",
    "    print('TRAIN:\\n'+classification_report(train_y_tfidf, clf_best.predict(train_X_tfidf), zero_division=1))\n",
    "    print('DEV:\\n'+classification_report(dev_y_tfidf, clf_best.predict(dev_X_tfidf), zero_division=1))\n",
    "\n",
    "    plot_confusion_matrix(dev_y_tfidf, clf_best.predict(dev_X_tfidf), classes=list(clf.classes_),\n",
    "                          title='Confusion matrix for training data')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "best_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08242658",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Random Forest on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd2075",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier(max_depth=40, n_estimators=200, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "np.random.seed(42)\n",
    "# parameters = {'max_depth':[np.random.randint(20) for i in range(6)]}\n",
    "# parameters = {'max_depth':[40], 'n_estimators':[np.random.randint(100, 300) for i in range(10)]}\n",
    "parameters = {'max_depth':[10, 20, 30], 'n_estimators':[50, 80, 100]}\n",
    "# parameters = {'max_depth': [3, 5, 10],\n",
    "#               'min_samples_split': [2, 5, 10]}\n",
    "tree = RandomForestClassifier(random_state=42)\n",
    "clf = GridSearchCV(tree, parameters, verbose=3)\n",
    "clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "clf.best_estimator_\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48573ce",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_params_values = [50, 80, 100, 120, 140, 200]\n",
    "# best_params_values = [50, 80, 100]\n",
    "best_params_values = [50, 100]\n",
    "best_f1_scores = []\n",
    "max_depths = [10, 20, 30]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    for best_param in best_params_values:\n",
    "        clf_best = RandomForestClassifier(max_depth=max_depth, n_estimators=best_param, random_state=0)\n",
    "        clf_best = clf_best.fit(train_X_tfidf, train_y_tfidf)\n",
    "        predicted = clf_best.predict(dev_X_tfidf)\n",
    "        f1_score = metrics.f1_score(dev_y_tfidf, predicted, average='micro')\n",
    "        dic = {\n",
    "            'max_depth': max_depth,\n",
    "            'n_estimators': best_param, \n",
    "            'f1_score': round(f1_score, 2)\n",
    "        }\n",
    "        best_f1_scores.append(dic)\n",
    "\n",
    "        print(f\"Classification report for max_depth={max_depth} & n_estimators={best_param}\\n\")\n",
    "        print('TRAIN:\\n'+classification_report(train_y_tfidf, clf_best.predict(train_X_tfidf), zero_division=1))\n",
    "        print('DEV:\\n'+classification_report(dev_y_tfidf, clf_best.predict(dev_X_tfidf), zero_division=1))\n",
    "        \n",
    "        plot_confusion_matrix(train_y_tfidf, clf_best.predict(train_X_tfidf), classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for train data')\n",
    "        plot_confusion_matrix(dev_y_tfidf, clf_best.predict(dev_X_tfidf), classes=list(clf.classes_),\n",
    "                              title='Confusion matrix for dev data')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    best_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bb713",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing with the best param models\n",
    "# best model = RandomForestClassifier(max_depth=30, n_estimators=100, random_state=42)\n",
    "clf = RandomForestClassifier(max_depth=30, n_estimators=100, random_state=42)\n",
    "clf = clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_tfidf, clf.predict(train_X_tfidf), zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_tfidf, clf.predict(dev_X_tfidf), zero_division=1))\n",
    "\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X_tfidf), classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf0912",
   "metadata": {},
   "source": [
    "#### SVC on TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89905f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parameters = {'kernel':['rbf', 'poly', 'sigmoid']}\n",
    "# # parameters = {'kernel':['linear'], 'C':[1, 3, 5, 7, 10]}\n",
    "# # parameters = {'C': [0.1, 1, 10, 100], \n",
    "# #               'gamma': [1, 0.1, 0.01, 0.001],\n",
    "# #               'kernel': ['rbf', 'poly', 'sigmoid']\n",
    "# #              }\n",
    "# svc = svm.SVC()\n",
    "# clf = GridSearchCV(svc, parameters, verbose=3)\n",
    "# clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "\n",
    "# # sorted(clf.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff36a35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing with rbf kernel\n",
    "clf = svm.SVC(kernel='rbf', random_state=42)\n",
    "clf = clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "\n",
    "train_preds = clf.predict(train_X_tfidf)\n",
    "dev_preds = clf.predict(dev_X_tfidf)\n",
    "print('TRAIN:\\n'+classification_report(train_y_tfidf, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_tfidf, dev_preds, zero_division=1))\n",
    "\n",
    "# Saving the best performing model on TF-IDF\n",
    "dump(clf, '../models/tfidf_best_model_svc_rbf_notebook.joblib')\n",
    "\n",
    "plot_confusion_matrix(train_y_tfidf, train_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "\n",
    "plot_confusion_matrix(dev_y_tfidf, dev_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for dev data')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003bf29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_tfidf = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ebc1b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing with poly kernel\n",
    "clf = svm.SVC(kernel='poly', random_state=42)\n",
    "clf = clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "\n",
    "train_preds = clf.predict(train_X_tfidf)\n",
    "dev_preds = clf.predict(dev_X_tfidf)\n",
    "print('TRAIN:\\n'+classification_report(train_y_tfidf, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_tfidf, dev_preds, zero_division=1))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(train_y_tfidf, train_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "\n",
    "plot_confusion_matrix(dev_y_tfidf, dev_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for dev data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65441ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with sigmoid kernel\n",
    "clf = svm.SVC(kernel='sigmoid', random_state=42)\n",
    "clf = clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "\n",
    "train_preds = clf.predict(train_X_tfidf)\n",
    "dev_preds = clf.predict(dev_X_tfidf)\n",
    "print('TRAIN:\\n'+classification_report(train_y_tfidf, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_tfidf, dev_preds, zero_division=1))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(train_y_tfidf, train_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "\n",
    "plot_confusion_matrix(dev_y_tfidf, dev_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for dev data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bde9d3",
   "metadata": {},
   "source": [
    "### Word embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d077c",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4176bed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "np.random.seed(42)\n",
    "# parameters = {'max_depth':[np.random.randint(20) for i in range(6)]}\n",
    "parameters = {'max_depth':[10, 12, 14, 16, 18, 20]}\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=42)\n",
    "clf = GridSearchCV(tree, parameters, verbose=3)\n",
    "clf.fit(train_X_wv, train_y_wv)\n",
    "print(clf.best_estimator_)\n",
    "clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e56c28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing with the best param models\n",
    "# clf = DecisionTreeClassifier(max_depth=18, random_state=42)\n",
    "\n",
    "best_params_values = [None, 5, 10, 15, 18, 20]\n",
    "best_f1_scores = []\n",
    "for best_param in best_params_values:\n",
    "    clf_best = DecisionTreeClassifier(max_depth=best_param, random_state=42)\n",
    "    clf_best = clf_best.fit(train_X_wv, train_y_wv)\n",
    "    preds_train = clf_best.predict(train_X_wv)\n",
    "    preds_dev = clf_best.predict(dev_X_wv)\n",
    "    train_f1_score = metrics.f1_score(train_y_wv, preds_train, average='micro')\n",
    "    dev_f1_score = metrics.f1_score(dev_y_wv, preds_dev, average='micro')\n",
    "    dic = {\n",
    "        'max_depth': best_param,\n",
    "        'train_f1_score': round(train_f1_score, 2),\n",
    "        'dev_f1_score': round(dev_f1_score, 2)\n",
    "    }\n",
    "    best_f1_scores.append(dic)\n",
    "    \n",
    "    print(f\"Classification report for max_depth = {best_param}\\n\")\n",
    "    print('TRAIN:\\n'+classification_report(train_y_wv, preds_train, zero_division=1))\n",
    "    print('DEV:\\n'+classification_report(dev_y_wv, preds_dev, zero_division=1))\n",
    "\n",
    "    \n",
    "best_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203a7d7d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b1b62e",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# RandomForestClassifier(max_depth=40, n_estimators=200, random_state=42)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "np.random.seed(42)\n",
    "# parameters = {'max_depth':[np.random.randint(20) for i in range(6)]}\n",
    "# parameters = {'max_depth':[40], 'n_estimators':[np.random.randint(100, 300) for i in range(10)]}\n",
    "parameters = {'max_depth':[10, 20, 30], 'n_estimators':[50, 80, 100]}\n",
    "# parameters = {'max_depth': [3, 5, 10],\n",
    "#               'min_samples_split': [2, 5, 10]}\n",
    "tree = RandomForestClassifier(random_state=42)\n",
    "clf = GridSearchCV(tree, parameters, verbose=3)\n",
    "clf.fit(train_X_wv, train_y_wv)\n",
    "print(clf.best_estimator_)\n",
    "best_param_idx = clf.cv_results_['rank_test_score'].tolist().index(2)\n",
    "clf.cv_results_['params'][best_param_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ff65c",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# best_params_values = [50, 80, 100, 120, 140, 200]\n",
    "# best_params_values = [50, 80, 100]\n",
    "n_trees = [50, 100]\n",
    "best_f1_scores = []\n",
    "# max_depths = [10, 20, 30]\n",
    "max_depths = [10, 30, 5]\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    for n_tree in n_trees:\n",
    "        clf_best = RandomForestClassifier(max_depth=max_depth, n_estimators=n_tree, random_state=0)\n",
    "        clf_best = clf_best.fit(train_X_wv, train_y_wv)\n",
    "        \n",
    "        preds_train = clf_best.predict(train_X_wv)\n",
    "        preds_dev = clf_best.predict(dev_X_wv)\n",
    "        \n",
    "        train_f1_score = metrics.f1_score(train_y_wv, preds_train, average='micro')\n",
    "        dev_f1_score = metrics.f1_score(dev_y_wv, preds_dev, average='micro')\n",
    "        \n",
    "        dic = {\n",
    "            'max_depth': max_depth,\n",
    "            'n_estimators': n_tree,\n",
    "            'train_f1_score': round(train_f1_score, 2),\n",
    "            'dev_f1_score': round(dev_f1_score, 2)\n",
    "        }\n",
    "        best_f1_scores.append(dic)\n",
    "\n",
    "        print(f\"Classification report for max_depth = {max_depth} & n_estimators = {n_tree}\\n\")\n",
    "        print('TRAIN:\\n'+classification_report(train_y_wv, preds_train, zero_division=1))\n",
    "        print('DEV:\\n'+classification_report(dev_y_wv, preds_dev, zero_division=1))\n",
    "\n",
    "\n",
    "best_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f335f15",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing with the best param models\n",
    "# best model = RandomForestClassifier(max_depth=30, n_estimators=100, random_state=42)\n",
    "clf = RandomForestClassifier(max_depth=30, n_estimators=100, random_state=42)\n",
    "clf = clf.fit(train_X_tfidf, train_y_tfidf)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_tfidf, clf.predict(train_X_tfidf), zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_tfidf, clf.predict(dev_X_tfidf), zero_division=1))\n",
    "\n",
    "plot_confusion_matrix(dev_spans_labels, clf.predict(dev_X_tfidf), classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075468b9",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fe27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing with rbf kernel\n",
    "clf = svm.SVC(kernel='rbf', random_state=42)\n",
    "clf = clf.fit(train_X_wv, train_y_wv)\n",
    "\n",
    "train_preds = clf.predict(train_X_wv)\n",
    "dev_preds = clf.predict(dev_X_wv)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_wv, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_wv, dev_preds, zero_division=1))\n",
    "\n",
    "# Saving the best performing model on TF-IDF\n",
    "dump(clf, '../models/word_embedding_best_model_svc_rbf_notebook.joblib')\n",
    "plot_confusion_matrix(train_y_tfidf, train_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "\n",
    "\n",
    "plot_confusion_matrix(dev_y_tfidf, dev_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for dev data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bbd6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_wv = clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87ecef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing with poly kernel\n",
    "clf = svm.SVC(kernel='poly', random_state=42)\n",
    "clf = clf.fit(train_X_wv, train_y_wv)\n",
    "\n",
    "train_preds = clf.predict(train_X_wv)\n",
    "dev_preds = clf.predict(dev_X_wv)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_wv, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_wv, dev_preds, zero_division=1))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(train_y_tfidf, train_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "\n",
    "plot_confusion_matrix(dev_y_tfidf, dev_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for dev data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c382e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with sigmoid kernel\n",
    "clf = svm.SVC(kernel='sigmoid', random_state=42)\n",
    "clf = clf.fit(train_X_wv, train_y_wv)\n",
    "\n",
    "train_preds = clf.predict(train_X_wv)\n",
    "dev_preds = clf.predict(dev_X_wv)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_wv, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_wv, dev_preds, zero_division=1))\n",
    "\n",
    "\n",
    "plot_confusion_matrix(train_y_tfidf, train_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for training data')\n",
    "\n",
    "plot_confusion_matrix(dev_y_tfidf, dev_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for dev data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4134b7",
   "metadata": {},
   "source": [
    "## Testing the Best model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d3c584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the best model on TF-IDF features\n",
    "# vectorizer = fasttext.load_model(\"../models/ft_word_embedding_model.bin\")\n",
    "# feature_vector = make_word_embedded_feature_vectors_and_labels(spans, vectorizer)\n",
    "best_model_tfidf = load('../models/tfidf_best_model_svc_rbf_notebook.joblib') \n",
    "clf = best_model_tfidf\n",
    "\n",
    "train_preds = clf.predict(train_X_tfidf)\n",
    "dev_preds = clf.predict(dev_X_tfidf)\n",
    "test_preds = clf.predict(test_X_tfidf)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_tfidf, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_tfidf, dev_preds, zero_division=1))\n",
    "print('TEST:\\n'+classification_report(test_y_tfidf, test_preds, zero_division=1))\n",
    "\n",
    "plot_confusion_matrix(train_y_tfidf, train_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for train data')\n",
    "\n",
    "plot_confusion_matrix(dev_y_tfidf, dev_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for dev data')\n",
    "\n",
    "plot_confusion_matrix(test_y_tfidf, test_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1c2639",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading the best model on word embedding features\n",
    "# vectorizer = fasttext.load_model(\"../models/ft_word_embedding_model.bin\")\n",
    "# feature_vector = make_word_embedded_feature_vectors_and_labels(spans, vectorizer)\n",
    "best_model_wv = load('../models/word_embedding_best_model_svc_rbf_notebook.joblib') \n",
    "clf = best_model_wv\n",
    "\n",
    "train_preds = clf.predict(train_X_wv)\n",
    "dev_preds = clf.predict(dev_X_wv)\n",
    "test_preds = clf.predict(test_X_wv)\n",
    "\n",
    "print('TRAIN:\\n'+classification_report(train_y_wv, train_preds, zero_division=1))\n",
    "print('DEV:\\n'+classification_report(dev_y_wv, dev_preds, zero_division=1))\n",
    "print('TEST:\\n'+classification_report(test_y_wv, test_preds, zero_division=1))\n",
    "\n",
    "plot_confusion_matrix(train_y_wv, train_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for train data')\n",
    "\n",
    "plot_confusion_matrix(dev_y_wv, dev_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for dev data')\n",
    "\n",
    "plot_confusion_matrix(test_y_wv, test_preds, classes=list(clf.classes_),\n",
    "                      title='Confusion matrix for test data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cab5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
